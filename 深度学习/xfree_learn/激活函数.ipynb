{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n",
      "sigmoid: torch.Size([8, 5])\n",
      "relu torch.Size([8, 5])\n",
      "tang torch.Size([8, 5])\n",
      "softmax torch.Size([8, 5])\n",
      "log_softmax torch.Size([8, 5])\n",
      "leaky_relu torch.Size([8, 5])\n",
      "elu torch.Size([8, 5])\n",
      "selu torch.Size([8, 5])\n",
      "softplus torch.Size([8, 5])\n",
      "mish torch.Size([8, 5])\n",
      "custom mish torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "layer = nn.Linear(in_features=16, out_features=5)\n",
    "x = torch.randn(size = (8,16))\n",
    "layer_output = layer(x)\n",
    "print(layer_output.size())\n",
    "# relu \n",
    "layer_output = F.relu(layer_output)\n",
    "print(\"relu\",layer_output.size())\n",
    "# mish\n",
    "layer_output = F.mish(layer_output)\n",
    "print(\"mish\",layer_output.size())\n",
    "def mish(x):\n",
    "    return x * F.tanh(F.softplus(x))\n",
    "# custom mish\n",
    "layer_output = mish(layer_output)\n",
    "print(\"custom mish\",layer_output.size())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sigmoid\n",
    "layer_output = F.sigmoid(layer_output)\n",
    "print(\"sigmoid:\",layer_output.size())\n",
    "\n",
    "# tanh\n",
    "layer_output = F.tanh(layer_output)\n",
    "print(\"tang\",layer_output.size())\n",
    "# softmax\n",
    "layer_output = F.softmax(layer_output,dim=1)\n",
    "print(\"softmax\",layer_output.size())\n",
    "# log_softmax\n",
    "layer_output = F.log_softmax(layer_output,dim=1)\n",
    "print(\"log_softmax\",layer_output.size())\n",
    "# leaky_relu\n",
    "layer_output = F.leaky_relu(layer_output)\n",
    "print(\"leaky_relu\",layer_output.size())\n",
    "# elu\n",
    "layer_output = F.elu(layer_output)\n",
    "print(\"elu\",layer_output.size())\n",
    "# selu\n",
    "layer_output = F.selu(layer_output)\n",
    "print(\"selu\",layer_output.size())\n",
    "# softplus\n",
    "layer_output = F.softplus(layer_output)\n",
    "print(\"softplus\",layer_output.size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

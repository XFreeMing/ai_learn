1
00:00:00,650 --> 00:00:08,090
In this video, we'll figure out how to get gradient descent to work with regularized linear regression. Let's jump in.

2
00:00:08,090 --> 00:00:14,130
Here Is a cost function we've come up with in the last video for regularized linear regression.

3
00:00:14,130 --> 00:00:32,250
The first part is the usual squared error cost function, and now you have this additional regularization term, where Lambda is the regularization parameter, and you'd like to find parameters w and b that minimize the regularized cost function.

4
00:00:32,250 --> 00:00:58,675
Previously we were using gradient descent for the original cost function, just the first term before we added that second regularization term, and previously, we had the following gradient descent algorithm, which is that we repeatedly update the parameters w, j, and b for j equals 1 through n according to this formula and b is also updated similarly.

5
00:00:58,675 --> 00:01:03,800
Again, Alpha is a very small positive number called the learning rate.

6
00:01:03,800 --> 00:01:13,500
In fact, the updates for a regularized linear regression look exactly the same, except that now the cost, J, is defined a bit differently.

7
00:01:13,500 --> 00:01:26,560
Previously the derivative of J with respect to w_j was given by this expression over here, and the derivative respect to b was given by this expression over here.

8
00:01:26,560 --> 00:01:43,475
Now that we've added this additional regularization term, the only thing that changes is that the expression for the derivative with respect to w_j ends up with one additional term, this plus Lambda over m times w_j.

9
00:01:43,475 --> 00:01:58,630
And in particular for the new definition of the cost function j, these two expressions over here, these are the new derivatives of J with respect to w_j and the derivative of J with respect to b.

10
00:01:58,630 --> 00:02:04,230
Recall that we don't regularize b, so we're not trying to shrink B.

11
00:02:04,230 --> 00:02:15,740
That's why the updated B remains the same as before, whereas the updated w changes because the regularization term causes us to try to shrink w_j.

12
00:02:16,010 --> 00:02:28,675
Let's take these definitions for the derivatives and put them back into the expression on the left to write out the gradient descent algorithm for regularized linear regression.

13
00:02:28,675 --> 00:02:36,940
To implement gradient descent for regularized linear regression, this is what you would have your code do.

14
00:02:36,940 --> 00:02:42,935
Here is the update for w_j, for j equals 1 through n, and here's the update for b.

15
00:02:42,935 --> 00:02:49,564
As usual, please remember to carry out simultaneous updates for all of these parameters.

16
00:02:49,564 --> 00:02:55,375
Now, in order for you to get this algorithm to work, this is all you need to know.

17
00:02:55,375 --> 00:03:09,685
But what I like to do in the remainder of this video is to go over some optional material to convey a slightly deeper intuition about what this formula is actually doing, as well as chat briefly about how these derivatives are derived.

18
00:03:09,685 --> 00:03:12,450
The rest of this video is completely optional.

19
00:03:12,450 --> 00:03:19,670
It's completely okay if you skip the rest of this video, but if you have a strong interests in math, then stick with me.

20
00:03:19,670 --> 00:03:29,600
It is always nice to hang out with you here, and through these equations, perhaps you can build a deeper intuition about what the math and what the derivatives are doing as well.

21
00:03:29,600 --> 00:03:37,040
Let's take a look. Let's look at the update rule for w_j and rewrite it in another way.

22
00:03:37,040 --> 00:03:49,035
We're updating w_j as 1 times w_j minus Alpha times Lambda over m times w_j.

23
00:03:49,035 --> 00:03:52,710
I've moved the term from the end to the front here.

24
00:03:52,710 --> 00:04:00,535
Then minus Alpha times 1 over m, and then the rest of that term over there.

25
00:04:00,535 --> 00:04:04,185
We just rearranged the terms a little bit.

26
00:04:04,185 --> 00:04:19,610
If we simplify, then we're saying that w_j is updated as w_j times 1 minus Alpha times Lambda over m, minus Alpha times this other term over here.

27
00:04:19,610 --> 00:04:27,680
You might recognize the second term as the usual gradient descent update for unregularized linear regression.

28
00:04:27,680 --> 00:04:37,130
This is the update for linear regression before we had regularization, and this is the term we saw in Week 2 of this course.

29
00:04:37,130 --> 00:04:52,875
The only change we add regularization is that instead of w_j being set to be equal to w_j minus Alpha times this term is now w times this number minus the usual update.

30
00:04:52,875 --> 00:04:56,460
This is what we had in Week 1 of this course.

31
00:04:56,460 --> 00:05:05,115
What is this first term over here? Well, Alpha is a very small positive number, say 0.01.

32
00:05:05,115 --> 00:05:10,625
Lambda is usually a small number, say 1 or maybe 10.

33
00:05:10,625 --> 00:05:17,925
Let's say lambda is 1 for this example and m is the training set size, say 50.

34
00:05:17,925 --> 00:05:43,700
When you multiply Alpha Lambda over m, say 0.01 times 1 divided by 50, this term ends up being a small positive number , say 0.0002, and thus, 1 minus Alpha Lambda over m is going to be a number just slightly less than 1, in this case, 0.9998.

35
00:05:43,700 --> 00:05:59,785
The effect of this term is that on every single iteration of gradient descent, you're taking w_j and multiplying it by 0.9998, that is by some numbers slightly less than one and for carrying out the usual update.

36
00:05:59,785 --> 00:06:13,075
What regularization is doing on every single iteration is you're multiplying w by a number slightly less than 1, and that has effect of shrinking the value of w_j just a little bit.

37
00:06:13,075 --> 00:06:24,985
This gives us another view on why regularization has the effect of shrinking the parameters w_j a little bit on every iteration, and so that's how regularization works.

38
00:06:24,985 --> 00:06:35,815
If you're curious about how these derivative terms were computed, I've just one last optional slide that goes through just a little bit of a calculation of the derivative term.

39
00:06:35,815 --> 00:06:44,795
Again, this slide and the rest of this video are completely optional, meaning you won't need any of this to do the practice labs and the quizzes.

40
00:06:44,795 --> 00:06:48,740
Let's step through quickly to derivative calculation.

41
00:06:48,740 --> 00:06:55,830
The derivative of J with respect to w_j looks like this.

42
00:07:05,630 --> 00:07:16,290
Recall that f of x for linear regression is defined as w dot x plus b or w dot product x plus b.

43
00:07:16,290 --> 00:07:45,800
It turns out that by the rules of calculus, the derivatives look like this, is 1 over 2m times the sum i equals 1 through m of w dot x plus b minus y times 2x_j plus the derivative of the regularization term, which is Lambda over 2m times 2 w_j.

44
00:07:45,800 --> 00:07:53,495
Notice that the second term does not have the summation term from j equals 1 through n anymore.

45
00:07:53,495 --> 00:07:59,065
The 2's cancel out here and here, and also here and here.

46
00:07:59,065 --> 00:08:04,570
It simplifies to this expression over here.

47
00:08:07,040 --> 00:08:17,350
Finally, remember that wx plus b is f of x, and so you can rewrite it as this expression down here.

48
00:08:17,350 --> 00:08:24,790
This is why this expression is used to compute the gradient in regularized linear regression.

49
00:08:24,790 --> 00:08:29,045
You now know how to implement regularized linear regression.

50
00:08:29,045 --> 00:08:35,095
Using this, you really reduce overfitting when you have a lot of features and a relatively small training set.

51
00:08:35,095 --> 00:08:40,390
This should let you get linear regression to work much better on many problems.

52
00:08:40,390 --> 00:08:49,010
In the next video, we'll take this regularization idea and apply it to logistic regression to avoid overfitting for logistic regression as well.

53
00:08:49,010 --> 00:08:52,110
Let's take a look at that in the next video.


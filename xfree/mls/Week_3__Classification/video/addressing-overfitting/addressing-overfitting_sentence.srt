1
00:00:01,400 --> 00:00:09,090
Later in this specialization, we'll talk about debugging and diagnosing things that can go wrong with learning algorithms.

2
00:00:09,090 --> 00:00:16,005
You'll also learn about specific tools to recognize when overfitting and underfitting may be occurring.

3
00:00:16,005 --> 00:00:21,870
But for now, when you think overfitting has occurred, lets talk about what you can do to address it.

4
00:00:21,870 --> 00:00:27,110
Let's say you fit a model and it has high variance, is overfit.

5
00:00:27,110 --> 00:00:31,375
Here's our overfit house price prediction model.

6
00:00:31,375 --> 00:00:38,680
One way to address this problem is to collect more training data, that's one option.

7
00:00:38,680 --> 00:00:53,770
If you're able to get more data, that is more training examples on sizes and prices of houses, then with the larger training set, the learning algorithm will learn to fit a function that is less wiggly.

8
00:00:53,770 --> 00:01:04,135
You can continue to fit a high order polynomial or some of the function with a lot of features, and if you have enough training examples, it will still do okay.

9
00:01:04,135 --> 00:01:11,700
To summarize, the number one tool you can use against overfitting is to get more training data.

10
00:01:11,700 --> 00:01:14,970
Now, getting more data isn't always an option.

11
00:01:14,970 --> 00:01:21,280
Maybe only so many houses have been sold in this location, so maybe there just isn't more data to be add.

12
00:01:21,280 --> 00:01:24,440
But when the data is available, this can work really well.

13
00:01:24,440 --> 00:01:30,785
A second option for addressing overfitting is to see if you can use fewer features.

14
00:01:30,785 --> 00:01:43,895
In the previous video, our models features included the size x, as well as the size squared, and this x squared, and x cubed and x^4 and so on.

15
00:01:43,895 --> 00:01:47,560
These were a lot of polynomial features.

16
00:01:47,560 --> 00:01:55,000
In that case, one way to reduce overfitting is to just not use so many of these polynomial features.

17
00:01:55,000 --> 00:01:57,655
But now let's look at a different example.

18
00:01:57,655 --> 00:02:12,910
Maybe you have a lot of different features of a house of which to try to predict its price, ranging from the size, number of bedrooms, number of floors, the age, average income of the neighborhood, and so on and so forth, total distance to the nearest coffee shop.

19
00:02:12,910 --> 00:02:23,695
It turns out that if you have a lot of features like these but don't have enough training data, then your learning algorithm may also overfit to your training set.

20
00:02:23,695 --> 00:02:35,900
Now instead of using all 100 features, if we were to pick just a subset of the most useful ones, maybe size, bedrooms, and the age of the house.

21
00:02:35,900 --> 00:02:45,815
If you think those are the most relevant features, then using just that smallest subset of features, you may find that your model no longer overfits as badly.

22
00:02:45,815 --> 00:02:52,240
Choosing the most appropriate set of features to use is sometimes also called feature selection.

23
00:02:52,240 --> 00:03:01,070
One way you could do so is to use your intuition to choose what you think is the best set of features, what's most relevant for predicting the price.

24
00:03:01,070 --> 00:03:12,875
Now, one disadvantage of feature selection is that by using only a subset of the features, the algorithm is throwing away some of the information that you have about the houses.

25
00:03:12,875 --> 00:03:20,125
For example, maybe all of these features, all 100 of them are actually useful for predicting the price of a house.

26
00:03:20,125 --> 00:03:25,820
Maybe you don't want to throw away some of the information by throwing away some of the features.

27
00:03:25,820 --> 00:03:35,150
Later in Course 2, you'll also see some algorithms for automatically choosing the most appropriate set of features to use for our prediction task.

28
00:03:35,150 --> 00:03:39,295
Now, this takes us to the third option for reducing overfitting.

29
00:03:39,295 --> 00:03:46,400
This technique, which we'll look at in even greater depth in the next video is called regularization.

30
00:03:46,400 --> 00:03:55,570
If you look at an overfit model, here's a model using polynomial features: x, x squared, x cubed, and so on.

31
00:03:55,570 --> 00:03:59,665
You find that the parameters are often relatively large.

32
00:03:59,665 --> 00:04:12,220
Now if you were to eliminate some of these features, say, if you were to eliminate the feature x4, that corresponds to setting this parameter to 0.

33
00:04:12,220 --> 00:04:20,515
So setting a parameter to 0 is equivalent to eliminating a feature, which is what we saw on the previous slide.

34
00:04:20,515 --> 00:04:31,825
It turns out that regularization is a way to more gently reduce the impacts of some of the features without doing something as harsh as eliminating it outright.

35
00:04:31,825 --> 00:04:43,505
What regularization does is encourage the learning algorithm to shrink the values of the parameters without necessarily demanding that the parameter is set to exactly 0.

36
00:04:43,505 --> 00:04:55,175
It turns out that even if you fit a higher order polynomial like this, so long as you can get the algorithm to use smaller parameter values: w1, w2, w3, w4.

37
00:04:55,175 --> 00:05:00,275
You end up with a curve that ends up fitting the training data much better.

38
00:05:00,275 --> 00:05:13,720
So what regularization does, is it lets you keep all of your features, but they just prevents the features from having an overly large effect, which is what sometimes can cause overfitting.

39
00:05:13,720 --> 00:05:23,125
By the way, by convention, we normally just reduce the size of the wj parameters, that is w1 through wn.

40
00:05:23,125 --> 00:05:31,370
It doesn't make a huge difference whether you regularize the parameter b as well, you could do so if you want or not if you don't.

41
00:05:31,370 --> 00:05:41,265
I usually don't and it's just fine to regularize w1, w2, all the way to wn, but not really encourage b to become smaller.

42
00:05:41,265 --> 00:05:47,035
In practice, it should make very little difference whether you also regularize b or not.

43
00:05:47,035 --> 00:05:54,275
To recap, these are the three ways you saw in this video for addressing overfitting.

44
00:05:54,275 --> 00:05:56,765
One, collect more data.

45
00:05:56,765 --> 00:06:01,615
If you can get more data, this can really help reduce overfitting.

46
00:06:01,615 --> 00:06:03,800
Sometimes that's not possible.

47
00:06:03,800 --> 00:06:11,735
In which case, some of the options are, two, try selecting and using only a subset of the features.

48
00:06:11,735 --> 00:06:16,315
You'll learn more about feature selection in Course 2.

49
00:06:16,315 --> 00:06:23,210
Three would be to reduce the size of the parameters using regularization.

50
00:06:23,210 --> 00:06:26,470
This will be the subject of the next video as well.

51
00:06:26,470 --> 00:06:29,675
Just for myself, I use regularization all the time.

52
00:06:29,675 --> 00:06:38,515
So this is a very useful technique for training learning algorithms, including neural networks specifically, which you'll see later in this specialization as well.

53
00:06:38,515 --> 00:06:43,820
I hope you'll also check out the optional lab on overfitting.

54
00:06:43,820 --> 00:06:52,660
In the lab, you'll be able to see different examples of overfitting and adjust those examples by clicking on options in the plots.

55
00:06:52,660 --> 00:07:00,835
You'll also be able to add your own data points by clicking on the plot and see how that changes the curve that is fit.

56
00:07:00,835 --> 00:07:13,105
You can also try examples for both regression and classification and you will change the degree of the polynomial to be x, x squared, x cubed, and so on.

57
00:07:13,105 --> 00:07:18,850
The lab also lets you play with two different options for addressing overfitting.

58
00:07:18,850 --> 00:07:30,790
You can add additional training data to reduce overfitting and you can also select which features to include or to exclude as another way to try to reduce overfitting.

59
00:07:30,790 --> 00:07:39,670
Please take a look at a lab, which I hope will help you build your intuition about overfitting as well as some methods for addressing it.

60
00:07:39,670 --> 00:07:45,650
In this video, you also saw the idea of regularization at a relatively high level.

61
00:07:45,650 --> 00:07:51,650
I realize that all of these details on regularization may not fully make sense to you yet.

62
00:07:51,650 --> 00:07:59,850
But in the next video, we'll start to formulate exactly how to apply regularization and exactly what regularization means.

63
00:07:59,850 --> 00:08:11,750
Then we'll start to figure out how to make this work with our learning algorithms to make linear regression and logistic regression, and in the future, other algorithms as well avoid overfitting.

64
00:08:11,750 --> 00:08:15,000
Let's take a look at that in the next video.


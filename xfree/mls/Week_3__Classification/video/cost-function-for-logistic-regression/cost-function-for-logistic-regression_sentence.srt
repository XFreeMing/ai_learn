1
00:00:00,800 --> 00:00:09,705
Remember that the cost function gives you a way to measure how well a specific set of parameters fits the training data.

2
00:00:09,705 --> 00:00:13,980
Thereby gives you a way to try to choose better parameters.

3
00:00:13,980 --> 00:00:21,810
In this video, we'll look at how the squared error cost function is not an ideal cost function for logistic regression.

4
00:00:21,810 --> 00:00:28,485
We'll take a look at a different cost function that can help us choose better parameters for logistic regression.

5
00:00:28,485 --> 00:00:33,370
Here's what the training set for our logistic regression model might look like.

6
00:00:33,370 --> 00:00:44,035
Where here each row might correspond to patients that was paying a visit to the doctor and one dealt with some diagnosis.

7
00:00:44,035 --> 00:00:50,425
As before, we'll use m to denote the number of training examples.

8
00:00:50,425 --> 00:01:01,745
Each training example has one or more features, such as the tumor size, the patient's age, and so on for a total of n features.

9
00:01:01,745 --> 00:01:06,085
Let's call the features X_1 through X_n.

10
00:01:06,085 --> 00:01:15,505
Since this is a binary classification task, the target label y takes on only two values, either 0 or 1.

11
00:01:15,505 --> 00:01:21,540
Finally, the logistic regression model is defined by this equation.

12
00:01:21,610 --> 00:01:35,060
The question you want to answer is, given this training set, how can you choose parameters w and b? Recall for linear regression, this is the squared error cost function.

13
00:01:35,060 --> 00:01:42,235
The only thing I've changed is that I put the one half inside the summation instead of outside the summation.

14
00:01:42,235 --> 00:01:50,330
You might remember that in the case of linear regression, where f of x is the linear function, w dot x plus b.

15
00:01:50,330 --> 00:01:56,560
The cost function looks like this, is a convex function or a bowl shape or hammer shape.

16
00:01:56,560 --> 00:02:05,260
Gradient descent will look like this, where you take one step, one step, and so on to converge at the global minimum.

17
00:02:05,260 --> 00:02:10,575
Now you could try to use the same cost function for logistic regression.

18
00:02:10,575 --> 00:02:25,585
But it turns out that if I were to write f of x equals 1 over 1 plus e to the negative wx plus b and plot the cost function using this value of f of x, then the cost will look like this.

19
00:02:25,585 --> 00:02:32,100
This becomes what's called a non-convex cost function is not convex.

20
00:02:32,100 --> 00:02:37,095
What this means is that if you were to try to use gradient descent.

21
00:02:37,095 --> 00:02:41,565
There are lots of local minima that you can get sucking.

22
00:02:41,565 --> 00:02:48,740
It turns out that for logistic regression, this squared error cost function is not a good choice.

23
00:02:48,740 --> 00:02:55,300
Instead, there will be a different cost function that can make the cost function convex again.

24
00:02:55,300 --> 00:03:00,105
The gradient descent can be guaranteed to converge to the global minimum.

25
00:03:00,105 --> 00:03:06,280
The only thing I've changed is that I put the one half inside the summation instead of outside the summation.

26
00:03:06,280 --> 00:03:10,980
This will make the math you see later on this slide a little bit simpler.

27
00:03:10,980 --> 00:03:16,040
In order to build a new cost function, one that we'll use for logistic regression.

28
00:03:16,040 --> 00:03:22,415
I'm going to change a little bit the definition of the cost function J of w and b.

29
00:03:22,415 --> 00:03:32,255
In particular, if you look inside this summation, let's call this term inside the loss on a single training example.

30
00:03:32,255 --> 00:03:47,350
I'm going to denote the loss via this capital L and as a function of the prediction of the learning algorithm, f of x as well as of the true label y.

31
00:03:47,350 --> 00:03:58,720
The loss given the predictor f of x and the true label y is equal in this case to 1.5 of the squared difference.

32
00:03:58,720 --> 00:04:12,740
We'll see shortly that by choosing a different form for this loss function, will be able to keep the overall cost function, which is 1 over n times the sum of these loss functions to be a convex function.

33
00:04:12,740 --> 00:04:23,610
Now, the loss function inputs f of x and the true label y and tells us how well we're doing on that example.

34
00:04:23,610 --> 00:04:30,620
I'm going to just write down here at the definition of the loss function we'll use for logistic regression.

35
00:04:30,620 --> 00:04:48,320
If the label y is equal to 1, then the loss is negative log of f of x and if the label y is equal to 0, then the loss is negative log of 1 minus f of x.

36
00:04:48,320 --> 00:04:55,115
Let's take a look at why this loss function hopefully makes sense.

37
00:04:55,115 --> 00:05:06,085
Let's first consider the case of y equals 1 and plot what this function looks like to gain some intuition about what this loss function is doing.

38
00:05:06,085 --> 00:05:21,680
Remember, the loss function measures how well you're doing on one training example and is by summing up the losses on all of the training examples that you then get, the cost function, which measures how well you're doing on the entire training set.

39
00:05:21,920 --> 00:05:32,465
If you plot log of f, it looks like this curve here, where f here is on the horizontal axis.

40
00:05:32,465 --> 00:05:41,510
A plot of a negative of the log of f looks like this, where we just flip the curve along the horizontal axis.

41
00:05:41,510 --> 00:05:50,140
Notice that it intersects the horizontal axis at f equals 1 and continues downward from there.

42
00:05:50,140 --> 00:05:54,315
Now, f is the output of logistic regression.

43
00:05:54,315 --> 00:06:02,325
Thus, f is always between zero and one because the output of logistic regression is always between zero and one.

44
00:06:02,325 --> 00:06:12,985
The only part of the function that's relevant is therefore this part over here, corresponding to f between 0 and 1.

45
00:06:12,985 --> 00:06:17,965
Let's zoom in and take a closer look at this part of the graph.

46
00:06:17,965 --> 00:06:28,210
If the algorithm predicts a probability close to 1 and the true label is 1, then the loss is very small.

47
00:06:28,210 --> 00:06:33,220
It's pretty much 0 because you're very close to the right answer.

48
00:06:33,220 --> 00:06:41,005
Now continue with the example of the true label y being 1, say everything is a malignant tumor.

49
00:06:41,005 --> 00:06:51,580
If the algorithm predicts 0.5, then the loss is at this point here, which is a bit higher but not that high.

50
00:06:51,580 --> 00:07:04,360
Whereas in contrast, if the algorithm were to have outputs at 0.1 if it thinks that there is only a 10 percent chance of the tumor being malignant but y really is 1.

51
00:07:04,360 --> 00:07:10,220
If really is malignant, then the loss is this much higher value over here.

52
00:07:10,380 --> 00:07:24,580
When y is equal to 1, the loss function incentivizes or nurtures, or helps push the algorithm to make more accurate predictions because the loss is lowest, when it predicts values close to 1.

53
00:07:24,580 --> 00:07:29,905
Now on this slide, we'll be looking at what the loss is when y is equal to 1.

54
00:07:29,905 --> 00:07:36,775
On this slide, let's look at the second part of the loss function corresponding to when y is equal to 0.

55
00:07:36,775 --> 00:07:42,805
In this case, the loss is negative log of 1 minus f of x.

56
00:07:42,805 --> 00:07:47,410
When this function is plotted, it actually looks like this.

57
00:07:47,410 --> 00:07:56,200
The range of f is limited to 0 to 1 because logistic regression only outputs values between 0 and 1.

58
00:07:56,200 --> 00:08:00,260
If we zoom in, this is what it looks like.

59
00:08:00,270 --> 00:08:12,835
In this plot, corresponding to y equals 0, the vertical axis shows the value of the loss for different values of f of x.

60
00:08:12,835 --> 00:08:31,435
When f is 0 or very close to 0, the loss is also going to be very small which means that if the true label is 0 and the model's prediction is very close to 0, well, you nearly got it right so the loss is appropriately very close to 0.

61
00:08:31,435 --> 00:08:40,120
The larger the value of f of x gets, the bigger the loss because the prediction is further from the true label 0.

62
00:08:40,120 --> 00:08:46,615
In fact, as that prediction approaches 1, the loss actually approaches infinity.

63
00:08:46,615 --> 00:09:06,565
Going back to the tumor prediction example just says if the model predicts that the patient's tumor is almost certain to be malignant, say, 99.9 percent chance of malignancy, that turns out to actually not be malignant, so y equals 0 then we penalize the model with a very high loss.

64
00:09:06,565 --> 00:09:18,535
In this case of y equals 0, so this is in the case of y equals 1 on the previous slide, the further the prediction f of x is away from the true value of y, the higher the loss.

65
00:09:18,535 --> 00:09:29,515
In fact, if f of x approaches 0, the loss here actually goes really large and in fact approaches infinity.

66
00:09:29,515 --> 00:09:38,170
When the true label is 1, the algorithm is strongly incentivized not to predict something too close to 0.

67
00:09:38,170 --> 00:09:44,500
In this video, you saw why the squared error cost function doesn't work well for logistic regression.

68
00:09:44,500 --> 00:09:57,055
We also defined the loss for a single training example and came up with a new definition for the loss function for logistic regression.

69
00:09:57,055 --> 00:10:09,265
It turns out that with this choice of loss function, the overall cost function will be convex and thus you can reliably use gradient descent to take you to the global minimum.

70
00:10:09,265 --> 00:10:14,425
Proving that this function is convex, it's beyond the scope of this cost.

71
00:10:14,425 --> 00:10:29,845
You may remember that the cost function is a function of the entire training set and is, therefore, the average or 1 over m times the sum of the loss function on the individual training examples.

72
00:10:29,845 --> 00:10:43,675
The cost on a certain set of parameters, w and b, is equal to 1 over m times the sum of all the training examples of the loss on the training examples.

73
00:10:43,675 --> 00:10:56,375
If you can find the value of the parameters, w and b, that minimizes this, then you'd have a pretty good set of values for the parameters w and b for logistic regression.

74
00:10:56,375 --> 00:11:11,965
In the upcoming optional lab, you'll get to take a look at how the squared error cost function doesn't work very well for classification, because you see that the surface plot results in a very wiggly costs surface with many local minima.

75
00:11:11,965 --> 00:11:17,500
Then you'll take a look at the new logistic loss function.

76
00:11:17,500 --> 00:11:26,485
As you can see here, this produces a nice and smooth convex surface plot that does not have all those local minima.

77
00:11:26,485 --> 00:11:31,250
Please take a look at the cost and the plots after this video.

78
00:11:32,220 --> 00:11:35,050
We've seen a lot in this video.

79
00:11:35,050 --> 00:11:45,415
In the next video, let's go back and take the loss function for a single train example and use that to define the overall cost function for the entire training set.

80
00:11:45,415 --> 00:11:56,170
We'll also figure out a simpler way to write out the cost function, which will then later allow us to run gradient descent to find good parameters for logistic regression.

81
00:11:56,170 --> 00:11:59,120
Let's go on to the next video.


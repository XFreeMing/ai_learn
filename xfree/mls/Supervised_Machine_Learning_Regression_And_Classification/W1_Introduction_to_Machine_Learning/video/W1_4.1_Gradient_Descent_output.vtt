WEBVTT

1
00:00:00.380 --> 00:00:13.725
Welcome back. In the last video, we saw visualizations of the cost function j and how you can try different choices of the parameters w and b and see what cost value they get you.

2
00:00:13.725 --> 00:00:23.890
It would be nice if we had a more systematic way to find the values of w and b, that results in the smallest possible cost, j of w, b.

3
00:00:23.890 --> 00:00:29.470
It turns out there's an algorithm called gradient descent that you can use to do that.

4
00:00:29.470 --> 00:00:42.940
Gradient descent is used all over the place in machine learning, not just for linear regression, but for training for example some of the most advanced neural network models, also called deep learning models.

5
00:00:42.940 --> 00:00:47.740
Deep learning models are something you learned about in the second course.

6
00:00:47.740 --> 00:00:55.085
Learning these two of gradient descent will set you up with one of the most important building blocks in machine learning.

7
00:00:55.085 --> 00:00:58.865
Here's an overview of what we'll do with gradient descent.

8
00:00:58.865 --> 00:01:05.070
You have the cost function j of w, b right here that you want to minimize.

9
00:01:05.070 --> 00:01:21.815
In the example we've seen so far, this is a cost function for linear regression, but it turns out that gradient descent is an algorithm that you can use to try to minimize any function, not just a cost function for linear regression.

10
00:01:21.815 --> 00:01:37.070
Just to make this discussion on gradient descent more general, it turns out that gradient descent applies to more general functions, including other cost functions that work with models that have more than two parameters.

11
00:01:37.070 --> 00:01:55.175
For instance, if you have a cost function J as a function of w_1, w_2 up to w_n and b, your objective is to minimize j over the parameters w_1 to w_n and b.

12
00:01:55.175 --> 00:02:05.930
In other words, you want to pick values for w_1 through w_n and b, that gives you the smallest possible value of j.

13
00:02:05.930 --> 00:02:14.470
It turns out that gradient descent is an algorithm that you can apply to try to minimize this cost function j as well.

14
00:02:14.470 --> 00:02:21.535
What you're going to do is just to start off with some initial guesses for w and b.

15
00:02:21.535 --> 00:02:30.310
In linear regression, it won't matter too much what the initial value are, so a common choice is to set them both to 0.

16
00:02:30.310 --> 00:02:35.645
For example, you can set w to 0 and b to 0 as the initial guess.

17
00:02:35.645 --> 00:02:52.720
With the gradient descent algorithm, what you're going to do is, you'll keep on changing the parameters w and b a bit every time to try to reduce the cost j of w, b until hopefully j settles at or near a minimum.

18
00:02:52.720 --> 00:03:05.070
One thing I should note is that for some functions j that may not be a bow shape or a hammock shape, it is possible for there to be more than one possible minimum.

19
00:03:05.200 --> 00:03:13.265
Let's take a look at an example of a more complex surface plot j to see what gradient is doing.

20
00:03:13.265 --> 00:03:17.060
This function is not a squared error cost function.

21
00:03:17.060 --> 00:03:24.655
For linear regression with the squared error cost function, you always end up with a bow shape or a hammock shape.

22
00:03:24.655 --> 00:03:30.860
But this is a type of cost function you might get if you're training a neural network model.

23
00:03:30.860 --> 00:03:38.420
Notice the axes, that is w and b on the bottom axis.

24
00:03:38.420 --> 00:03:51.395
For different values of w and b, you get different points on this surface, j of w, b, where the height of the surface at some point is the value of the cost function.

25
00:03:51.395 --> 00:04:04.765
Now, let's imagine that this surface plot is actually a view of a slightly hilly outdoor park or a golf course where the high points are hills and the low points are valleys like so.

26
00:04:04.765 --> 00:04:12.220
I'd like you to imagine if you will, that you are physically standing at this point on the hill.

27
00:04:12.220 --> 00:04:21.080
If it helps you to relax, imagine that there's lots of really nice green grass and butterflies and flowers is a really nice hill.

28
00:04:21.080 --> 00:04:31.170
Your goal is to start up here and get to the bottom of one of these valleys as efficiently as possible.

29
00:04:31.250 --> 00:04:49.990
What the gradient descent algorithm does is, you're going to spin around 360 degrees and look around and ask yourself, if I were to take a tiny little baby step in one direction, and I want to go downhill as quickly as possible to or one of these valleys.

30
00:04:49.990 --> 00:05:08.615
What direction do I choose to take that baby step? Well, if you want to walk down this hill as efficiently as possible, it turns out that if you're standing at this point in the hill and you look around, you will notice that the best direction to take your next step downhill is roughly that direction.

31
00:05:08.615 --> 00:05:13.090
Mathematically, this is the direction of steepest descent.

32
00:05:13.090 --> 00:05:22.930
It means that when you take a tiny baby little step, this takes you downhill faster than a tiny little baby step you could have taken in any other direction.

33
00:05:22.930 --> 00:05:29.205
After taking this first step, you're now at this point on the hill over here.

34
00:05:29.205 --> 00:05:31.440
Now let's repeat the process.

35
00:05:31.440 --> 00:05:52.460
Standing at this new point, you're going to again spin around 360 degrees and ask yourself, in what direction will I take the next little baby step in order to move downhill? If you do that and take another step, you end up moving a bit in that direction and you can keep going.

36
00:05:52.460 --> 00:05:59.170
From this new point, you can again look around and decide what direction would take you downhill most quickly.

37
00:05:59.170 --> 00:06:09.005
Take another step, another step, and so on, until you find yourself at the bottom of this valley, at this local minimum, right here.

38
00:06:09.005 --> 00:06:13.540
What you just did was go through multiple steps of gradient descent.

39
00:06:13.540 --> 00:06:18.020
It turns out, gradient descent has an interesting property.

40
00:06:18.020 --> 00:06:26.860
Remember that you can choose a starting point at the surface by choosing starting values for the parameters w and b.

41
00:06:26.860 --> 00:06:33.610
When you perform gradient descent a moment ago, you had started at this point over here.

42
00:06:33.610 --> 00:06:47.300
Now, imagine if you try gradient descent again, but this time you choose a different starting point by choosing parameters that place your starting point just a couple of steps to the right over here.

43
00:06:47.300 --> 00:06:57.035
If you then repeat the gradient descent process, which means you look around, take a little step in the direction of steepest ascent so you end up here.

44
00:06:57.035 --> 00:07:01.120
Then you again look around, take another step, and so on.

45
00:07:01.120 --> 00:07:13.420
If you were to run gradient descent this second time, starting just a couple steps in the right of where we did it the first time, then you end up in a totally different valley.

46
00:07:13.420 --> 00:07:17.530
This different minimum over here on the right.

47
00:07:17.530 --> 00:07:23.735
The bottoms of both the first and the second valleys are called local minima.

48
00:07:23.735 --> 00:07:40.975
Because if you start going down the first valley, gradient descent won't lead you to the second valley, and the same is true if you started going down the second valley, you stay in that second minimum and not find your way into the first local minimum.

49
00:07:40.975 --> 00:07:47.720
This is an interesting property of the gradient descent algorithm, and you see more about this later.

50
00:07:47.720 --> 00:07:53.155
In this video, you saw how gradient descent helps you go downhill.

51
00:07:53.155 --> 00:08:00.725
In the next video, let's look at the mathematical expressions that you can implement to make gradient descent work.

52
00:08:00.725 --> 00:08:03.390
Let's go on to the next video.


WEBVTT

1
00:00:01.400 --> 00:00:10.255
Now let's dive more deeply in gradient descent to gain better intuition about what it's doing and why it might make sense.

2
00:00:10.255 --> 00:00:14.990
Here's the gradient descent algorithm that you saw in the previous video.

3
00:00:14.990 --> 00:00:20.965
As a reminder, this variable, this Greek symbol Alpha, is the learning rate.

4
00:00:20.965 --> 00:00:28.505
The learning rate controls how big of a step you take when updating the model's parameters, w and b.

5
00:00:28.505 --> 00:00:34.875
This term here, this d over dw, this is a derivative term.

6
00:00:34.875 --> 00:00:40.410
By convention in math, this d is written with this funny font here.

7
00:00:40.410 --> 00:00:50.540
In case anyone watching this has PhD in math or is an expert in multivariate calculus, they may be wondering, that's not the derivative, that's the partial derivative.

8
 --> 00:00:50.540
Yes, they be right.

9
00:00:50.540 --> 00:00:56.300
But for the purposes of implementing a machine learning algorithm, I'm just going to call it derivative.

10
00:00:56.300 --> 00:00:59.560
Don't worry about these little distinctions.

11
00:00:59.560 --> 00:01:15.335
What we're going to focus on now is get more intuition about what this learning rate and what this derivative are doing and why when multiplied together like this, it results in updates to parameters w and b.

12
00:01:15.335 --> 00:01:25.930
That makes sense. In order to do this let's use a slightly simpler example where we work on minimizing just one parameter.

13
00:01:25.930 --> 00:01:33.490
Let's say that you have a cost function J of just one parameter w with w is a number.

14
00:01:33.490 --> 00:01:37.095
This means the gradient descent now looks like this.

15
00:01:37.095 --> 00:02:07.830
W is updated to w minus the learning rate Alpha times d over dw of J of w. You're trying to minimize the cost by adjusting the parameter w. This is like our previous example where we had temporarily set b equal to 0 with one parameter w instead of two, you can look at two-dimensional graphs of the cost function j, instead of three dimensional graphs.

16
00:02:07.830 --> 00:02:30.660
Let's look at what gradient descent does on just function J of w. Here on the horizontal axis is parameter w, and on the vertical axis is the cost j of w. Now less initialized gradient descent with some starting value for w. Let's initialize it at this location.

17
00:02:30.660 --> 00:02:51.560
Imagine that you start off at this point right here on the function J, what gradient descent will do is it will update w to be w minus learning rate Alpha times d over dw of J of w. Let's look at what this derivative term here means.

18
00:02:51.560 --> 00:03:03.545
A way to think about the derivative at this point on the line is to draw a tangent line, which is a straight line that touches this curve at that point.

19
00:03:03.545 --> 00:03:10.120
Enough, the slope of this line is the derivative of the function j at this point.

20
00:03:10.120 --> 00:03:14.645
To get the slope, you can draw a little triangle like this.

21
00:03:14.645 --> 00:03:21.020
If you compute the height divided by the width of this triangle, that is the slope.

22
00:03:21.020 --> 00:03:39.100
For example, this slope might be 2 over 1, for instance and when the tangent line is pointing up and to the right, the slope is positive, which means that this derivative is a positive number, so is greater than 0.

23
00:03:39.100 --> 00:03:46.895
The updated w is going to be w minus the learning rate times some positive number.

24
00:03:46.895 --> 00:03:50.165
The learning rate is always a positive number.

25
00:03:50.165 --> 00:03:58.270
If you take w minus a positive number, you end up with a new value for w, that's smaller.

26
00:03:58.270 --> 00:04:20.105
On the graph, you're moving to the left, you're decreasing the value of w. You may notice that this is the right thing to do if your goal is to decrease the cost J, because when we move towards the left on this curve, the cost j decreases, and you're getting closer to the minimum for J, which is over here.

27
00:04:20.105 --> 00:04:25.125
So far, gradient descent, seems to be doing the right thing.

28
00:04:25.125 --> 00:04:28.485
Now, let's look at another example.

29
00:04:28.485 --> 00:04:36.050
Let's take the same function j of w as above, and now let's say that you initialized gradient descent at a different location.

30
00:04:36.050 --> 00:04:41.920
Say by choosing a starting value for w that's over here on the left.

31
00:04:41.920 --> 00:04:44.910
That's this point of the function j.

32
00:04:44.910 --> 00:05:00.580
Now, the derivative term, remember is d over dw of J of w, and when we look at the tangent line at this point over here, the slope of this line is a derivative of J at this point.

33
00:05:00.580 --> 00:05:04.790
But this tangent line is sloping down into the right.

34
00:05:04.790 --> 00:05:09.410
This lines sloping down into the right has a negative slope.

35
00:05:09.410 --> 00:05:13.975
In other words, the derivative of J at this point is a negative number.

36
00:05:13.975 --> 00:05:28.955
For instance, if you draw a triangle, then the height like this is negative 2 and the width is 1, the slope is negative 2 divided by 1, which is negative 2, which is a negative number.

37
00:05:28.955 --> 00:05:35.890
When you update w, you get w minus the learning rate times a negative number.

38
00:05:35.890 --> 00:05:40.915
This means you subtract from w, a negative number.

39
00:05:40.915 --> 00:06:09.485
But subtracting a negative number means adding a positive number, and so you end up increasing w. Because subtracting a negative number is the same as adding a positive number to w. This step of gradient descent causes w to increase, which means you're moving to the right of the graph and your cost J has decrease down to here.

40
00:06:09.485 --> 00:06:16.685
Again, it looks like gradient descent is doing something reasonable, is getting you closer to the minimum.

41
00:06:16.685 --> 00:06:31.075
Hopefully, these last two examples show some of the intuition behind what a derivative term is doing and why this host gradient descent change w to get you closer to the minimum.

42
00:06:31.075 --> 00:06:37.970
I hope this video gave you some sense for why the derivative term in gradient descent makes sense.

43
00:06:37.970 --> 00:06:43.115
One other key quantity in the gradient descent algorithm is the learning rate Alpha.

44
00:06:43.115 --> 00:07:01.380
How do you choose Alpha? What happens if it's too small or what happens when it's too big? In the next video, let's take a deeper look at the parameter Alpha to help build intuitions about what it does, as well as how to make a good choice for a good value of Alpha for your implementation of gradient descent.


WEBVTT

1
00:00:02.040 --> 00:00:06.090
Let's see what happens when you run gradient descent for linear regression.

2
00:00:06.090 --> 00:00:08.640
Let's go see the algorithm in action.

3
00:00:08.640 --> 00:00:23.140
Here's a plot of the model and data on the upper left and a contour plot of the cost function on the upper right and at the bottom is the surface plot of the same cost function.

4
00:00:23.140 --> 00:00:35.740
Often w and b will both be initialized to 0, but for this demonstration, lets initialized w = -0.1 and b = 900.

5
00:00:35.740 --> 00:00:43.361
So this corresponds to f(x) = -0.1x + 900.

6
00:00:44.540 --> 00:01:02.451
Now, if we take one step using gradient descent, we ended up going from this point of the cost function out here to this point just down and to the right and notice that the straight line fit is also changed a bit.

7
00:01:04.140 --> 00:01:05.161
Let's take another step.

8
00:01:06.840 --> 00:01:14.561
The cost function has now moved to this third and again the function f(x) has also changed a bit.

9
00:01:15.740 --> 00:01:21.440
As you take more of these steps, the cost is decreasing at each update.

10
00:01:21.440 --> 00:01:26.261
So the parameters w and b are following this trajectory.

11
00:01:28.140 --> 00:01:40.240
And if you look on the left, you get this corresponding straight line fit that fits the data better and better until we've reached the global minimum.

12
00:01:40.240 --> 00:01:47.640
The global minimum corresponds to this straight line fit, which is a relatively good fit to the data.

13
00:01:47.640 --> 00:01:50.240
I mean, isn't that cool.

14
00:01:50.240 --> 00:01:58.240
And so that's gradient descent and we're going to use this to fit a model to the holding data.

15
00:01:58.240 --> 00:02:06.640
And you can now use this f(x) model to predict the price of your clients house or anyone else's house.

16
00:02:06.640 --> 00:02:21.255
For instance, if your friend's house size is 1250 square feet, you can now read off the value and predict that maybe they could get, I don't know, $250,000 for the house.

17
00:02:21.255 --> 00:02:27.179
To be more precise, this gradient descent process is called batch gradient descent.

18
00:02:27.179 --> 00:02:39.651
The term batch gradient descent refers to the fact that on every step of gradient descent, we're looking at all of the training examples, instead of just a subset of the training data.

19
00:02:41.140 --> 00:02:50.440
So in computing grading descent, when computing derivatives, when computing the sum from i =1 to m.

20
00:02:50.440 --> 00:02:58.940
And bash gradient descent is looking at the entire batch of training examples at each update.

21
00:02:58.940 --> 00:03:06.840
I know that bash grading percent may not be the most intuitive name, but this is what people in the machine learning community call it.

22
00:03:06.840 --> 00:03:12.494
If you've heard of the newsletter The Batch, that's published by DeepLearning.AI.

23
00:03:12.494 --> 00:03:17.051
The newsletter The batch was also named for this concept in machine learning.

24
00:03:18.340 --> 00:03:29.840
And then it turns out that there are other versions of gradient descent that do not look at the entire training set, but instead looks at smaller subsets of the training data at each update step.

25
00:03:29.840 --> 00:03:33.351
But we'll use batch gradient descent for linear regression.

26
00:03:34.440 --> 00:03:36.590
So that's it for linear regression.

27
00:03:36.590 --> 00:03:40.470
Congratulations on getting through your first machine learning model.

28
00:03:40.470 --> 00:03:45.555
I hope you go and celebrate or I don't know maybe take a nap in your hammock.

29
00:03:45.555 --> 00:03:48.680
In the optional lab that follows this video.

30
00:03:48.680 --> 00:03:54.440
You'll see a review of the gradient descent algorithm as was how to implement it in code.

31
00:03:54.440 --> 00:04:01.340
You'll also see a plot that shows how the cost decreases as you continue training more iterations.

32
00:04:01.340 --> 00:04:13.240
And you'll also see a contour plot, seeing how the cost gets closer to the global minimum as gradient descent finds better and better values for the parameters w and b.

33
00:04:13.240 --> 00:04:16.440
So remember that to do the optional lab.

34
00:04:16.440 --> 00:04:19.300
You just need to read and run this code.

35
00:04:19.300 --> 00:04:24.810
You will need to write any code yourself and I hope you take a few moments to do that.

36
00:04:24.810 --> 00:04:35.061
And also become familiar with the gradient descent code because this will help you to implement this and similar algorithms in the future yourself.

37
00:04:36.440 --> 00:04:43.540
Thanks for sticking with me through the end of this last video for the first week and congratulations for making it all the way here.

38
00:04:43.540 --> 00:04:47.112
You're on your way to becoming a machine learning person.

39
00:04:47.112 --> 00:04:50.930
In addition to the optional labs, if you haven't done so yet.

40
00:04:50.930 --> 00:04:58.540
I hope you also check out the practice quizzes, which are a nice way that you can double check your own understanding of the concepts.

41
00:04:58.540 --> 00:05:02.340
It's also totally fine, if you don't get them all right the first time.

42
00:05:02.340 --> 00:05:07.940
And you can also take the quizzes multiple times until you get the score that you want.

43
00:05:07.940 --> 00:05:15.840
You now know how to implement linear regression with one variable and that brings us to the close of this week.

44
00:05:15.840 --> 00:05:26.040
Next week, we'll learn to make linear regression much more powerful instead of one feature like size of a house, you learn how to get it to work with lots of features.

45
00:05:26.040 --> 00:05:29.940
You'll also learn how to get it to fit nonlinear curves.

46
00:05:29.940 --> 00:05:34.740
These improvements will make the algorithm much more useful and valuable.

47
00:05:34.740 --> 00:05:43.140
Lastly, we'll also go over some practical tips that will really hope for getting linear regression to work on practical applications.

48
00:05:43.140 --> 00:05:47.251
I'm really happy to have you here with me in this class and I look forward to seeing you next week.


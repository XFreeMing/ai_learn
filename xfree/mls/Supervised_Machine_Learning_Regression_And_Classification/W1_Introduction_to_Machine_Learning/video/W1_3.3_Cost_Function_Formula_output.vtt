WEBVTT

1
00:00:00.770 --> 00:00:07.935
In order to implement linear regression > 为了实现线性回归 the first key step is first to > 第一个关键步骤是首先 define something called a cost function.

2
00:00:07.935 --> 00:00:17.160
> 定义一个称为成本函数的东西。 This is something we'll build in this video, > 这是我们将在本视频中构建的东西， and the cost function will tell us how well > 成本函数将告诉我们模型的表现如何 the model is doing so that > 模型的表现如何 we can try to get it to do better.

3
00:00:17.160 --> 00:00:18.825
> 我们可以尝试让它做得更好。 Let's look at what this means.

4
00:00:18.825 --> 00:00:26.040
> 让我们看看这意味着什么。 Recall that you have a training set that contains > 回想一下，你有一个包含 input features x and output targets y.

5
00:00:26.040 --> 00:00:36.525
> 输入特征x和输出目标y的训练集。 The model you're going to use to fit this training set > 你要用来拟合这个训练集的模型 is this linear function f_w, > 是这个线性函数f_w， b of x equals to w times x plus b.

6
00:00:36.525 --> 00:00:44.250
> b of x等于w乘以x加上b。 To introduce a little bit more terminology the w > 为了引入更多术语，w and b are called the parameters of the model.

7
00:00:44.250 --> 00:00:53.385
> 和b被称为模型的参数。 In machine learning parameters of the model are > 在机器学习中，模型的参数是 the variables you can adjust during > 您可以在训练中调整的变量 training in order to improve the model.

8
00:00:53.385 --> 00:01:07.650
> 以改进模型。 Sometimes you also hear the parameters w and b > 有时你也会听到参数w和b referred to as coefficients or as weights.plots > 被称为系数或权重。 Now let's take a look at what > 现在让我们看看这些参数w和b these parameters w and b do.

9
00:01:07.650 --> 00:01:17.050
> 做了什么。 Depending on the values you've chosen for w and > 根据您为w和b选择的值 b you get a different function f of x, > b，您会得到一个不同的函数f of x， which generates a different line on the graph.

10
00:01:17.050 --> 00:01:24.390
> 这在图表上生成不同的线。 Remember that we can write f of x as > 记住我们可以将f of x写成 a shorthand for f_w, b of x.

11
00:01:24.390 --> 00:01:29.325
> f_w，b of x的简写。 We're going to take a look at some plots > 我们将看一些图表 of f of x on a chart.

12
00:01:29.325 --> 00:01:50.400
> f of x在图表上。 Maybe you're already familiar > 也许你已经熟悉了 with drawing lines on charts, > 在图表上画线， but even if this is a review for you, > 但即使这对你来说是一个复习， I hope this will help you build intuition > 我希望这能帮助你建立直觉 on how w and b the parameters > 关于参数w和b如何 determine f. When w is equal to 0 and b is equal to 1.5, > 确定f。当w等于0，b等于1.5， then f looks like this horizontal line.

13
00:01:50.400 --> 00:02:00.355
> 那么f看起来像这条水平线。 In this case, the function f of x is 0 times x > 在这种情况下，函数f of x是0乘以x plus 1.5 so f is always a constant value.

14
00:02:00.355 --> 00:02:08.505
> 加1.5，所以f总是一个常数值。 It always predicts 1.5 for > 它总是预测1.5 the estimated value of y.

15
00:02:08.505 --> 00:02:17.970
Y hat is always equal to b > y的估计值。Y hat总是等于b and here b is also called > 这里的b也被称为 the y intercept because that's where it > y截距，因为那是它 crosses the vertical axis or the y axis on this graph.

16
00:02:17.970 --> 00:02:28.515
> 在这个图表上穿过垂直轴或y轴。 As a second example, > 作为第二个例子， if w is 0.5 and b is equal 0, > 如果w是0.5，b等于0， then f of x is 0.5 times x.

17
00:02:28.515 --> 00:02:38.020
> 那么f of x是0.5乘以x。 When x is 0, > 当x等于0， the prediction is also 0, > 预测也是0， and when x is 2, > 当x等于2， then the prediction is 0.5 times 2, which is 1.

18
00:02:38.020 --> 00:02:45.975
> 那么预测是0.5乘以2，即1。 You get a line that looks like this and notice that > 你得到一条看起来像这样的线，并注意 the slope is 0.5 divided by 1.

19
00:02:45.975 --> 00:02:52.695
> 斜率是0.5除以1。 The value of w gives you the slope > w的值给出了斜率 of the line, which is 0.5.

20
00:02:52.695 --> 00:03:14.285
> 线的斜率，即0.5。 Finally, if w equals 0.5 and b equals 1, > 最后，如果w等于0.5，b等于1， then f of x is 0.5 times x plus 1 and when x is 0, > 那么f of x是0.5乘以x加1，当x等于0， then f of x equals b, > 那么f of x等于b， which is 1 so the line intersects > 即1，所以线与 the vertical axis at b, the y intercept.

21
00:03:14.285 --> 00:03:20.860
> 垂直轴相交在b，y截距。 Also when x is 2, > 当x等于2， then f of x is 2, > 那么f of x是2， so the line looks like this.

22
00:03:20.860 --> 00:03:28.675
> 所以线看起来像这样。 Again, this slope is 0.5 divided by > 再次，这个斜率是0.5除以 1 so the value of w gives you the slope which is 0.5.

23
00:03:28.675 --> 00:03:33.415
> 1，所以w的值给出了斜率，即0.5。 Recall that you have > 回想一下，你有 a training set like the one shown here.

24
00:03:33.415 --> 00:03:43.930
> 一个像这样的训练集。 With linear regression, > 使用线性回归， what you want to do is to choose > 你想做的是选择 values for the parameters w and > 参数w和b的值 b so that the straight line you get from > 使得从 the function f somehow fits the data well.

25
00:03:43.930 --> 00:03:46.715
> 函数f得到的直线与数据很好地契合。 Like maybe this line shown here.

26
00:03:46.715 --> 00:04:04.955
> 就像这里显示的线一样。 When I see that the line fits the data visually, > 当我看到线与数据视觉上契合时， you can think of this to mean that the line > 你可以认为这意味着线 defined by f is roughly passing > 由f定义的大致通过 through or somewhere close to the training examples > 通过或接近训练示例的线 as compared to other possible lines > 与其他可能的线相比 that are not as close to these points.

27
00:04:04.955 --> 00:04:20.180
> 不如这些点接近。 Just to remind you of some notation, > 只是提醒你一些符号， a training example like this point > 一个像这个点的训练示例 here is defined by x superscript i, > 这里由x上标i定义， y superscript i where y is the target.

28
00:04:20.180 --> 00:04:34.575
> y上标i，其中y是目标。 For a given input x^i, > 对于给定的输入x^i， the function f also makes a predictive value for > 函数f还为 y and a value that it predicts to > y做出预测的值 y is y hat i shown here.

29
00:04:34.575 --> 00:04:41.130
> y是这里显示的y hat i。 For our choice of a model f of x^i is w times x^i plus b.

30
00:04:41.130 --> 00:04:58.390
> 对于我们选择的模型f of x^i是w乘以x^i加上b。 Stated differently, the prediction y hat i > 换句话说，预测y hat i is f of wb of x^i where > 是f of wb of x^i，其中 for the model we're using f > 对于我们使用的模型f of x^i is equal to wx^i plus b.

31
00:04:58.930 --> 00:05:16.860
> of x^i等于wx^i加b。 Now the question is how do you find values for > 现在的问题是如何找到 w and b so that the prediction y hat i is > w和b的值，使得预测y hat i close to the true target y^i for > 接近真实目标y^i many or maybe all training examples x^i, y^i.

32
00:05:16.860 --> 00:05:24.430
> 许多或可能所有训练示例x^i，y^i。 To answer that question, > 为了回答这个问题， let's first take a look at how to > 让我们首先看一下如何 measure how well a line fits the training data.

33
00:05:24.430 --> 00:05:28.555
> 测量一条线如何适应训练数据。 To do that, we're going to construct a cost function.

34
00:05:28.555 --> 00:05:39.095
> 为此，我们将构建一个成本函数。 The cost function takes the prediction y hat and compares > 成本函数接受预测y hat并比较 it to the target y by taking y hat minus y.

35
00:05:39.095 --> 00:05:47.175
> 它与目标y，通过y hat减y。 This difference is called the error, > 这个差异被称为错误， we're measuring how far off to > 我们正在测量多远 prediction is from the target.

36
00:05:47.175 --> 00:05:52.265
> 预测与目标的差距。 Next, let's computes the square of this error.

37
00:05:52.265 --> 00:05:59.065
> 接下来，让我们计算这个错误的平方。 Also, we're going to want to compute this term for > 此外，我们还要计算这个术语 different training examples i in the training set.

38
00:05:59.065 --> 00:06:05.220
> 训练集中不同的训练示例i。 When measuring the error, > 在测量错误时， for example i, > 例如i， we'll compute this squared error term.

39
00:06:05.220 --> 00:06:09.815
> 我们将计算这个平方误差项。 Finally, we want to measure > 最后，我们想要测量 the error across the entire training set.

40
00:06:09.815 --> 00:06:13.700
> 整个训练集的错误。 In particular, let's sum up the squared errors like this.

41
00:06:13.700 --> 00:06:25.700
> 特别是，让我们这样总结平方误差。 We'll sum from i equals 1,2, > 我们将从i等于1,2开始求和， 3 all the way up to > 3一直到 m and remember that m is the number of training examples, > m，记住m是训练示例的数量， which is 47 for this dataset.

42
00:06:25.700 --> 00:06:32.915
> 对于这个数据集，这是47。 Notice that if we have more training examples m is > 请注意，如果我们有更多的训练示例m larger and your cost function > 更大，你的成本函数 will calculate a bigger number.

43
00:06:32.915 --> 00:06:35.765
> 将计算一个更大的数字。 This is summing over more examples.

44
00:06:35.765 --> 00:06:54.600
> 这是对更多示例求和。 To build a cost function that > 要构建一个成本函数 doesn't automatically get bigger > 不会自动变得更大 as the training set size gets larger by convention, > 随着训练集大小的增加而变大，按照惯例， we will compute the average squared error instead of > 我们将计算平均平方误差而不是 the total squared error and we do > 总平方误差，我们这样做 that by dividing by m like this.

45
00:06:54.650 --> 00:06:58.680
> 通过这样做来除以m。 We're nearly there.

46
 --> 00:06:58.680
Just one last thing.

47
00:06:58.680 --> 00:07:17.130
> 我们快到了。最后一件事。 By convention, > 按照惯例， the cost function that machine learning people use > 机器学习人员使用的成本函数 actually divides by 2 times m. The extra division > 实际上除以2乘以m。额外的除法 by 2 is just meant to make some of > 2的额外除法只是为了使一些 our later calculations look neater, > 我们后来的计算看起来更整洁， but the cost function still works whether you > 但成本函数仍然有效，无论你 include this division by 2 or not.

48
00:07:17.130 --> 00:07:27.470
> 包括这个除以2的除法还是不包括。 This expression right here is > 这里的表达式是 the cost function and we're going to write > 成本函数，我们将写 J of wb to refer to the cost function.

49
00:07:27.470 --> 00:07:36.800
> J of wb来指代成本函数。 This is also called the squared error cost function, > 这也被称为平方误差成本函数， and it's called this because you're taking > 这样称呼是因为你正在 the square of these error terms.

50
00:07:36.800 --> 00:07:56.375
> 这些错误项的平方。 In machine learning different people > 在机器学习中，不同的人 will use different cost functions > 将使用不同的成本函数 for different applications, > 用于不同的应用， but the squared error cost function is by far the most > 但平方误差成本函数是迄今为止最常用的 commonly used one for > 用于 linear regression and for that matter, > 线性回归和此外， for all regression problems where it > 对于所有回归问题，它 seems to give good results for many applications.

51
00:07:56.375 --> 00:08:05.615
> 似乎为许多应用提供了良好的结果。 Just as a reminder, the prediction y hat > 只是作为提醒，预测y hat is equal to the outputs of the model f at x.

52
00:08:05.615 --> 00:08:23.050
> 等于模型f在x处的输出。 We can rewrite the cost function J of > 我们可以重写成本函数J of wb as 1 over 2m times > wb为1除以2m乘以 the sum from i equals 1 to m of f > 从i等于1到m的f的总和 of x^i minus y^i the quantity squared.

53
00:08:23.050 --> 00:08:29.630
> x^i减y^i的数量平方。 Eventually we're going to want to find values of > 最终，我们将要找到 w and b that make the cost function small.

54
00:08:29.630 --> 00:08:38.105
> 使成本函数变小的w和b的值。 But before going there, > 但在那之前， let's first gain more intuition about what > 让我们首先更多地了解 J of wb is really computing.

55
00:08:38.105 --> 00:08:43.880
> J of wb真正计算的是什么。 At this point you might be thinking we've done > 此时，你可能会认为我们已经完成了 a whole lot of math to define the cost function.

56
00:08:43.880 --> 00:09:01.605
> 定义成本函数的大量数学。 But what exactly is it doing? > 但它到底在做什么？ Let's go on to the next video where we'll step > 让我们继续下一个视频，我们将迈出 through one example of what the cost function > 通过一个例子，成本函数 is really computing that I hope will > 真正计算的是我希望 help you build intuition about what it > 帮助你建立直觉，它 means if J of wb is large versus if the cost j is small.

57
00:09:01.605 --> 00:09:04.510
> 如果J of wb很大，与成本j很小相比。 Let's go on to the next video.


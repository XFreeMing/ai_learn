1
00:00:01,910 --> 00:00:08,240
You've learned about gradient descents about multiple linear regression and also vectorization.

2
00:00:08,240 --> 00:00:15,280
Let's put it all together to implement gradient descent for multiple linear regression with vectorization. This would be cool.

3
00:00:15,280 --> 00:00:18,805
Let's quickly review what multiple linear regression look like.

4
00:00:18,805 --> 00:00:24,790
Using our previous notation, let's see how you can write it more succinctly using vector notation.

5
00:00:24,790 --> 00:00:29,000
We have parameters w_1 to w_n as well as b.

6
00:00:29,000 --> 00:00:44,520
But instead of thinking of w_1 to w_n as separate numbers, that is separate parameters, let's start to collect all of the w's into a vector w so that now w is a vector of length n.

7
00:00:44,520 --> 00:00:53,935
We're just going to think of the parameters of this model as a vector w, as well as b, where b is still a number same as before.

8
00:00:53,935 --> 00:01:10,555
Whereas before we had to find multiple linear regression like this, now using vector notation, we can write the model as f_w, b of x equals the vector w dot product with the vector x plus b.

9
00:01:10,555 --> 00:01:14,985
Remember that this dot here means.product.

10
00:01:14,985 --> 00:01:21,050
Our cost function can be defined as J of w_1 through w_n, b.

11
00:01:21,050 --> 00:01:36,500
But instead of just thinking of J as a function of these and different parameters w_j as well as b, we're going to write J as a function of parameter vector w and the number b.

12
00:01:36,500 --> 00:01:49,865
This w_1 through w_n is replaced by this vector W and J now takes this input of vector w and a number b and returns a number.

13
00:01:49,865 --> 00:01:52,170
Here's what gradient descent looks like.

14
00:01:52,170 --> 00:02:06,300
We're going to repeatedly update each parameter w_j to be w_j minus Alpha times the derivative of the cost J, where J has parameters w_1 through w_n and b.

15
00:02:06,300 --> 00:02:13,045
Once again, we just write this as J of vector w and number b.

16
00:02:13,045 --> 00:02:20,935
Let's see what this looks like when you implement gradient descent and in particular, let's take a look at the derivative term.

17
00:02:20,935 --> 00:02:28,820
We'll see that gradient descent becomes just a little bit different with multiple features compared to just one feature.

18
00:02:28,820 --> 00:02:33,305
Here's what we had when we had gradient descent with one feature.

19
00:02:33,305 --> 00:02:41,725
We had an update rule for w and a separate update rule for b. Hopefully, these look familiar to you.

20
00:02:41,725 --> 00:02:57,575
This term here is the derivative of the cost function J with respect to the parameter w. Similarly, we have an update rule for parameter b, with univariate regression, we had only one feature.

21
00:02:57,575 --> 00:03:02,205
We call that feature xi without any subscript.

22
00:03:02,205 --> 00:03:08,845
Now, here's a new notation for where we have n features, where n is two or more.

23
00:03:08,845 --> 00:03:12,185
We get this update rule for gradient descent.

24
00:03:12,185 --> 00:03:26,755
Update w_1 to be w_1 minus Alpha times this expression here and this formula is actually the derivative of the cost J with respect to w_1.

25
00:03:26,755 --> 00:03:35,795
The formula for the derivative of J with respect to w_1 on the right looks very similar to the case of one feature on the left.

26
00:03:35,795 --> 00:03:41,645
The error term still takes a prediction f of x minus the target y.

27
00:03:41,645 --> 00:04:03,965
One difference is that w and x are now vectors and just as w on the left has now become w_1 here on the right, xi here on the left is now instead xi _1 here on the right and this is just for J equals 1.

28
00:04:03,965 --> 00:04:23,680
For multiple linear regression, we have J ranging from 1 through n and so we'll update the parameters w_1, w_2, all the way up to w_n, and then as before, we'll update b.

29
00:04:23,680 --> 00:04:29,285
If you implement this, you get gradient descent for multiple regression.

30
00:04:29,285 --> 00:04:33,695
That's it for gradient descent for multiple regression.

31
00:04:33,695 --> 00:04:46,840
Before moving on from this video, I want to make a quick aside or a quick side note on an alternative way for finding w and b for linear regression.

32
00:04:46,840 --> 00:04:50,565
This method is called the normal equation.

33
00:04:50,565 --> 00:05:15,225
Whereas it turns out gradient descent is a great method for minimizing the cost function J to find w and b, there is one other algorithm that works only for linear regression and pretty much none of the other algorithms you see in this specialization for solving for w and b and this other method does not need an iterative gradient descent algorithm.

34
00:05:15,225 --> 00:05:26,440
Called the normal equation method, it turns out to be possible to use an advanced linear algebra library to just solve for w and b all in one goal without iterations.

35
00:05:26,440 --> 00:05:44,030
Some disadvantages of the normal equation method are; first unlike gradient descent, this is not generalized to other learning algorithms, such as the logistic regression algorithm that you'll learn about next week or the neural networks or other algorithms you see later in this specialization.

36
00:05:44,030 --> 00:05:50,105
The normal equation method is also quite slow if the number of features and this large.

37
00:05:50,105 --> 00:06:08,150
Almost no machine learning practitioners should implement the normal equation method themselves but if you're using a mature machine learning library and call linear regression, there is a chance that on the backend, it'll be using this to solve for w and b.

38
00:06:08,150 --> 00:06:14,530
If you're ever in the job interview and hear the term normal equation, that's what this refers to.

39
00:06:14,530 --> 00:06:18,140
Don't worry about the details of how the normal equation works.

40
00:06:18,140 --> 00:06:26,470
Just be aware that some machine learning libraries may use this complicated method in the back-end to solve for w and b.

41
00:06:26,470 --> 00:06:35,635
But for most learning algorithms, including how you implement linear regression yourself, gradient descents offer a better way to get the job done.

42
00:06:35,635 --> 00:06:47,365
In the optional lab that follows this video, you'll see how to define a multiple regression model encode and also how to calculate the prediction f of x.

43
00:06:47,365 --> 00:06:55,250
You'll also see how to calculate the cost and implement gradient descent for a multiple linear regression model.

44
00:06:55,250 --> 00:06:58,715
This will be using Python's NumPy library.

45
00:06:58,715 --> 00:07:15,595
If any of the code looks very new, that's okay but you should feel free also to take a look at the previous optional lab that introduces NumPy and vectorization for a refresher of NumPy functions and how to implement those in code.

46
00:07:15,595 --> 00:07:19,470
That's it. You now know multiple linear regression.

47
00:07:19,470 --> 00:07:25,680
This is probably the single most widely used learning algorithm in the world today. But there's more.

48
00:07:25,680 --> 00:07:35,565
With just a few tricks such as picking and scaling features appropriately and also choosing the learning rate alpha appropriately, you'd really make this work much better.

49
00:07:35,565 --> 00:07:38,045
Just a few more videos to go for this week.

50
00:07:38,045 --> 00:07:45,810
Let's go on to the next video to see those little tricks that will help you make multiple linear regression work much better.


1
00:00:03,020 --> 00:00:08,905
Let's look at some more visualizations of w and b. Here's one example.

2
00:00:08,905 --> 00:00:14,400
Over here, you have a particular point on the graph j.

3
00:00:14,400 --> 00:00:22,470
For this point, w equals about negative 0.15 and b equals about 800.

4
00:00:22,470 --> 00:00:30,090
This point corresponds to one pair of values for w and b that use a particular cost j.

5
00:00:30,090 --> 00:00:40,495
In fact, this booklet pair of values for w and b corresponds to this function f of x, which is this line you can see on the left.

6
00:00:40,495 --> 00:00:53,770
This line intersects the vertical axis at 800 because b equals 800 and the slope of the line is negative 0.15, because w equals negative 0.15.

7
00:00:53,770 --> 00:01:01,180
Now, if you look at the data points in the training set, you may notice that this line is not a good fit to the data.

8
00:01:01,180 --> 00:01:15,785
For this function f of x, with these values of w and b, many of the predictions for the value of y are quite far from the actual target value of y that is in the training data.

9
00:01:15,785 --> 00:01:27,370
Because this line is not a good fit, if you look at the graph of j, the cost of this line is out here, which is pretty far from the minimum.

10
00:01:27,370 --> 00:01:34,260
There's a pretty high cost because this choice of w and b is just not that good a fit to the training set.

11
00:01:34,310 --> 00:01:41,180
Now, let's look at another example with a different choice of w and b.

12
00:01:41,180 --> 00:01:48,985
Now, here's another function that is still not a great fit for the data, but maybe slightly less bad.

13
00:01:48,985 --> 00:01:56,755
This points here represents the cost for this booklet pair of w and b that creates that line.

14
00:01:56,755 --> 00:02:03,640
The value of w is equal to 0 and the value b is about 360.

15
00:02:03,640 --> 00:02:13,655
This pair of parameters corresponds to this function, which is a flat line, because f of x equals 0 times x plus 360.

16
00:02:13,655 --> 00:02:15,520
I hope that makes sense.

17
00:02:15,520 --> 00:02:18,635
Let's look at yet another example.

18
00:02:18,635 --> 00:02:25,550
Here's one more choice for w and b, and with these values, you end up with this line f of x.

19
00:02:25,550 --> 00:02:32,620
Again, not a great fit to the data, is actually further away from the minimum compared to the previous example.

20
00:02:32,620 --> 00:02:38,250
Remember that the minimum is at the center of that smallest ellipse.

21
00:02:38,250 --> 00:02:46,670
Last example, if you look at f of x on the left, this looks like a pretty good fit to the training set.

22
00:02:46,670 --> 00:02:59,795
You can see on the right, this point representing the cost is very close to the center of the smaller ellipse, it's not quite exactly the minimum, but it's pretty close.

23
00:02:59,795 --> 00:03:06,340
For this value of w and b, you get to this line, f of x.

24
00:03:06,340 --> 00:03:18,280
You can see that if you measure the vertical distances between the data points and the predicted values on the straight line, you'd get the error for each data point.

25
00:03:18,280 --> 00:03:30,370
The sum of squared errors for all of these data points is pretty close to the minimum possible sum of squared errors among all possible straight line fits.

26
00:03:30,370 --> 00:04:00,935
I hope that by looking at these figures, you can get a better sense of how different choices of the parameters affect the line f of x and how this corresponds to different values for the cost j, and hopefully you can see how the better fit lines correspond to points on the graph of j that are closer to the minimum possible cost for this cost function j of w and b.

27
00:04:00,935 --> 00:04:18,400
In the optional lab that follows this video, you'll get to run some codes and remember all the code is given, so you just need to hit Shift Enter to run it and take a look at it and the lab will show you how the cost function is implemented in code.

28
00:04:18,400 --> 00:04:29,255
Given a small training set and different choices for the parameters, you'll be able to see how the cost varies depending on how well the model fits the data.

29
00:04:29,255 --> 00:04:35,070
In the optional lab, you also can play with in interactive console plot. Check this out.

30
00:04:35,070 --> 00:04:45,105
You can use your mouse cursor to click anywhere on the contour plot and you will see the straight line defined by the values you chose for the parameters w and b.

31
00:04:45,105 --> 00:04:51,425
You'll see a dot up here also on the 3D surface plot showing the cost.

32
00:04:51,425 --> 00:05:04,210
Finally, the optional lab also has a 3D surface plot that you can manually rotate and spin around using your mouse cursor to take a better look at what the cost function looks like.

33
00:05:04,210 --> 00:05:07,310
I hope you'll enjoy playing with the optional lab.

34
00:05:07,310 --> 00:05:21,265
Now in linear regression, rather than having to manually try to read a contour plot for the best value for w and b, which isn't really a good procedure and also won't work once we get to more complex machine learning models.

35
00:05:21,265 --> 00:05:31,895
What you really want is an efficient algorithm that you can write in code for automatically finding the values of parameters w and b they give you the best fit line.

36
00:05:31,895 --> 00:05:34,655
That minimizes the cost function j.

37
00:05:34,655 --> 00:05:38,530
There is an algorithm for doing this called gradient descent.

38
00:05:38,530 --> 00:05:42,830
This algorithm is one of the most important algorithms in machine learning.

39
00:05:42,830 --> 00:05:53,365
Gradient descent and variations on gradient descent are used to train, not just linear regression, but some of the biggest and most complex models in all of AI.

40
00:05:53,365 --> 00:06:00,540
Let's go to the next video to dive into this really important algorithm called gradient descent.


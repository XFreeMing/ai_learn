1
00:00:01,210 --> 00:00:09,359
The choice of the learning rate, alpha will have a huge impact on the efficiency of your implementation of gradient descent.

2
00:00:09,359 --> 00:00:15,632
And if alpha, the learning rate is chosen poorly rate of descent may not even work at all.

3
00:00:15,632 --> 00:00:19,449
In this video, let's take a deeper look at the learning rate.

4
00:00:19,449 --> 00:00:26,153
This will also help you choose better learning rates for your implementations of gradient descent.

5
00:00:26,153 --> 00:00:29,547
So here again, is the gradient descent rule.

6
00:00:29,547 --> 00:00:35,699
W is updated to be W minus the learning rate, alpha times the derivative term.

7
00:00:35,699 --> 00:00:38,637
To learn more about what the learning rate alpha is doing.

8
00:00:38,637 --> 00:00:46,100
Let's see what could happen if the learning rate alpha is either too small or if it is too large.

9
00:00:46,100 --> 00:00:49,053
For the case where the learning rate is too small.

10
00:00:49,053 --> 00:00:56,149
Here's a graph where the horizontal axis is W and the vertical axis is the cost J.

11
00:00:56,149 --> 00:01:01,328
And here's the graph of the function J of W.

12
00:01:01,328 --> 00:01:07,717
Let's start grading descent at this point here, if the learning rate is too small.

13
00:01:07,717 --> 00:01:14,910
Then what happens is that you multiply your derivative term by some really, really small number.

14
00:01:14,910 --> 00:01:18,045
So you're going to be multiplying by number alpha.

15
00:01:18,045 --> 00:01:23,103
That's really small, like 0.0000001.

16
00:01:23,103 --> 00:01:28,172
And so you end up taking a very small baby step like that.

17
00:01:28,172 --> 00:01:33,906
Then from this point you're going to take another tiny tiny little baby step.

18
00:01:33,906 --> 00:01:40,261
But because the learning rate is so small, the second step is also just minuscule.

19
00:01:40,261 --> 00:01:47,087
The outcome of this process is that you do end up decreasing the cost J but incredibly slowly.

20
00:01:47,087 --> 00:01:54,452
So, here's another step and another step, another tiny step until you finally approach the minimum.

21
00:01:54,452 --> 00:01:59,204
But as you may notice you're going to need a lot of steps to get to the minimum.

22
00:01:59,204 --> 00:02:07,700
So to summarize if the learning rate is too small, then gradient descents will work, but it will be slow.

23
00:02:07,700 --> 00:02:12,828
It will take a very long time because it's going to take these tiny tiny baby steps.

24
00:02:12,828 --> 00:02:17,637
And it's going to need a lot of steps before it gets anywhere close to the minimum.

25
00:02:17,637 --> 00:02:20,694
Now, let's look at a different case.

26
00:02:20,694 --> 00:02:26,552
What happens if the learning rate is too large? Here's another graph of the cost function.

27
00:02:26,552 --> 00:02:32,015
And let's say we start grating descent with W at this value here.

28
00:02:32,015 --> 00:02:37,177
So it's actually already pretty close to the minimum.

29
00:02:37,177 --> 00:02:40,234
So the decorative points to the right.

30
00:02:40,234 --> 00:02:51,570
But if the learning rate is too large then you update W very giant step to be all the way over here.

31
00:02:51,570 --> 00:02:56,303
And that's this point here on the function J.

32
00:02:56,303 --> 00:03:01,799
So you move from this point on the left, all the way to this point on the right.

33
00:03:01,799 --> 00:03:04,498
And now the cost has actually gotten worse.

34
00:03:04,498 --> 00:03:13,569
It has increased because it started out at this value here and after one step, it actually increased to this value here.

35
00:03:13,569 --> 00:03:22,234
Now the derivative at this new point says to decrease W but when the learning rate is too big.

36
00:03:22,234 --> 00:03:27,626
Then you may take a huge step going from here all the way out here.

37
00:03:27,626 --> 00:03:32,656
So now you've gotten to this point here and again, if the learning rate is too big.

38
00:03:32,656 --> 00:03:38,945
Then you take another huge step with an acceleration and way overshoot the minimum again.

39
00:03:38,945 --> 00:03:44,048
So now you're at this point on the right and one more time you do another update.

40
00:03:44,048 --> 00:03:51,161
And end up all the way here and so you're now at this point here.

41
00:03:51,161 --> 00:03:56,930
So as you may notice you're actually getting further and further away from the minimum.

42
00:03:56,930 --> 00:04:05,672
So if the learning rate is too large, then creating the sense may overshoot and may never reach the minimum.

43
00:04:05,672 --> 00:04:14,587
And another way to say that is that great intersect may fail to converge and may even diverge.

44
00:04:14,587 --> 00:04:23,477
So, here's another question, you may be wondering one of your parameter W is already at this point here.

45
00:04:23,477 --> 00:04:29,280
So that your cost J is already at a local minimum.

46
00:04:29,280 --> 00:04:38,722
What do you think? One step of gradient descent will do if you've already reached a minimum? So this is a tricky one.

47
00:04:38,722 --> 00:04:43,557
When I was first learning this stuff, it actually took me a long time to figure it out.

48
00:04:43,557 --> 00:04:45,772
But let's work through this together.

49
00:04:45,772 --> 00:04:49,849
Let's suppose you have some cost function J.

50
00:04:49,849 --> 00:05:02,864
And the one you see here isn't a square error cost function and this cost function has two local minima corresponding to the two valleys that you see here.

51
00:05:02,864 --> 00:05:13,095
Now let's suppose that after some number of steps of gradient descent, your parameter W is over here, say equal to five.

52
00:05:13,095 --> 00:05:16,562
And so this is the current value of W.

53
00:05:16,562 --> 00:05:20,272
This means that you're at this point on the cost function J.

54
00:05:20,272 --> 00:05:28,034
And that happens to be a local minimum, turns out if you draw attention to the function at this point.

55
00:05:28,034 --> 00:05:32,700
The slope of this line is zero and thus the derivative term.

56
00:05:32,700 --> 00:05:37,168
Here is equal to zero for the current value of W.

57
00:05:37,168 --> 00:05:45,641
And so you're grading descent update becomes W is updated to W minus the learning rate times zero.

58
00:05:45,641 --> 00:05:50,701
We're here that's because the derivative term is equal to zero.

59
00:05:50,701 --> 00:05:56,746
And this is the same as saying let's set W to be equal to W.

60
00:05:56,746 --> 00:06:04,254
So this means that if you're already at a local minimum, gradient descent leaves W unchanged.

61
00:06:04,254 --> 00:06:10,553
Because it just updates the new value of W to be the exact same old value of W.

62
00:06:10,553 --> 00:06:15,421
So concretely, let's say if the current value of W is five.

63
00:06:15,421 --> 00:06:28,727
And alpha is 0.1 after one iteration, you update W as W minus alpha times zero and it is still equal to five.

64
00:06:28,727 --> 00:06:37,480
So if your parameters have already brought you to a local minimum, then further gradient descent steps to absolutely nothing.

65
00:06:37,480 --> 00:06:43,953
It doesn't change the parameters which is what you want because it keeps the solution at that local minimum.

66
00:06:43,953 --> 00:06:51,508
This also explains why gradient descent can reach a local minimum, even with a fixed learning rate alpha.

67
00:06:51,508 --> 00:06:57,092
Here's what I mean, to illustrate this, let's look at another example.

68
00:06:57,092 --> 00:07:02,195
Here's the cost function J of W that we want to minimize.

69
00:07:02,195 --> 00:07:07,161
Let's initialize gradient descent up here at this point.

70
00:07:07,161 --> 00:07:13,762
If we take one update step, maybe it will take us to that point.

71
00:07:13,762 --> 00:07:21,289
And because this derivative is pretty large, grading, descent takes a relatively big step right.

72
00:07:21,289 --> 00:07:26,467
Now, we're at this second point where we take another step.

73
00:07:26,467 --> 00:07:31,119
And you may notice that the slope is not as steep as it was at the first point.

74
00:07:31,119 --> 00:07:33,885
So the derivative isn't as large.

75
00:07:33,885 --> 00:07:39,903
And so the next update step will not be as large as that first step.

76
00:07:39,903 --> 00:07:47,528
Now, read this third point here and the derivative is smaller than it was at the previous step.

77
00:07:47,528 --> 00:07:52,731
And will take an even smaller step as we approach the minimum.

78
00:07:52,731 --> 00:07:56,484
The decorative gets closer and closer to zero.

79
00:07:56,484 --> 00:08:04,850
So as we run gradient descent, eventually we're taking very small steps until you finally reach a local minimum.

80
00:08:04,850 --> 00:08:13,045
So just to recap, as we get nearer a local minimum gradient descent will automatically take smaller steps.

81
00:08:13,045 --> 00:08:19,586
And that's because as we approach the local minimum, the derivative automatically gets smaller.

82
00:08:19,586 --> 00:08:24,314
And that means the update steps also automatically gets smaller.

83
00:08:24,314 --> 00:08:28,573
Even if the learning rate alpha is kept at some fixed value.

84
00:08:28,573 --> 00:08:35,460
So that's the gradient descent algorithm, you can use it to try to minimize any cost function J.

85
00:08:35,460 --> 00:08:41,570
Not just the mean squared error cost function that we're using for the new regression.

86
00:08:41,570 --> 00:08:50,099
In the next video, we're going to take the function J and set that back to be exactly the linear regression models cost function.

87
00:08:50,099 --> 00:08:54,315
The mean squared error cost function that we come up with earlier.

88
00:08:54,315 --> 00:09:03,051
And putting together great in dissent with this cost function that will give you your first learning algorithm, the linear regression algorithm.


1
00:00:00,830 --> 00:00:10,095
Previously, you took a look at the linear regression model and then the cost function, and then the gradient descent algorithm.

2
00:00:10,095 --> 00:00:19,000
In this video, we're going to pull out together and use the squared error cost function for the linear regression model with gradient descent.

3
00:00:19,000 --> 00:00:26,865
This will allow us to train the linear regression model to fit a straight line to achieve the training data.

4
 --> 00:00:26,865
Let's get to it.

5
00:00:26,865 --> 00:00:30,210
Here's the linear regression model.

6
00:00:30,210 --> 00:00:34,675
To the right is the squared error cost function.

7
00:00:34,675 --> 00:00:38,150
Below is the gradient descent algorithm.

8
00:00:38,150 --> 00:00:45,215
It turns out if you calculate these derivatives, these are the terms you would get.

9
00:00:45,215 --> 00:01:03,020
The derivative with respect to W is this 1 over m, sum of i equals 1 through m. Then the error term, that is the difference between the predicted and the actual values times the input feature xi.

10
00:01:03,020 --> 00:01:15,550
The derivative with respect to b is this formula over here, which looks the same as the equation above, except that it doesn't have that xi term at the end.

11
00:01:15,550 --> 00:01:24,170
If you use these formulas to compute these two derivatives and implements gradient descent this way, it will work.

12
00:01:24,170 --> 00:01:31,130
Now, you may be wondering, where did I get these formulas from? They're derived using calculus.

13
00:01:31,130 --> 00:01:36,715
If you want to see the full derivation, I'll quickly run through the derivation on the next slide.

14
00:01:36,715 --> 00:01:41,460
But if you don't remember or aren't interested in the calculus, don't worry about it.

15
00:01:41,460 --> 00:01:50,215
You can skip the materials on the next slide entirely and still be able to implement gradient descent and finish this class and everything will work just fine.

16
00:01:50,215 --> 00:02:01,475
In this slide, which is one of the most mathematical slide of the entire specialization, and again is completely optional, we'll show you how to calculate the derivative terms.

17
00:02:01,475 --> 00:02:03,795
Let's start with the first term.

18
00:02:03,795 --> 00:02:18,710
The derivative of the cost function J with respect to w. We'll start by plugging in the definition of the cost function J. J of WP is this.

19
00:02:18,710 --> 00:02:25,255
1 over 2m times this sum of the squared error terms.

20
00:02:25,255 --> 00:02:41,500
Now remember also that f of wb of X^i is equal to this term over here, which is WX^i plus b.

21
00:02:41,500 --> 00:02:54,725
What we would like to do is compute the derivative, also called the partial derivative with respect to w of this equation right here on the right.

22
00:02:54,725 --> 00:03:06,770
If you taken a calculus class before, and again is totally fine if you haven't, you may know that by the rules of calculus, the derivative is equal to this term over here.

23
00:03:06,770 --> 00:03:18,610
Which is why the two here and two here cancel out, leaving us with this equation that you saw on the previous slide.

24
00:03:18,610 --> 00:03:29,060
This is why we had to find the cost function with the 1.5 earlier this week is because it makes the partial derivative neater.

25
00:03:29,060 --> 00:03:33,950
It cancels out the two that appears from computing the derivative.

26
00:03:33,950 --> 00:03:39,680
For the other derivative with respect to b, this is quite similar.

27
00:03:39,680 --> 00:03:49,510
I can write it out like this, and once again, plugging the definition of f of X^i, giving this equation.

28
00:03:49,510 --> 00:03:58,635
By the rules of calculus, this is equal to this where there's no X^i anymore at the end.

29
00:03:58,635 --> 00:04:07,000
The 2's cancel one small and you end up with this expression for the derivative with respect to b.

30
00:04:07,000 --> 00:04:11,150
Now you have these two expressions for the derivatives.

31
00:04:11,150 --> 00:04:15,850
You can plug them into the gradient descent algorithm.

32
00:04:15,850 --> 00:04:20,105
Here's the gradient descent algorithm for linear regression.

33
00:04:20,105 --> 00:04:26,230
You repeatedly carry out these updates to w and b until convergence.

34
00:04:26,230 --> 00:04:35,500
Remember that this f of x is a linear regression model, so as equal to w times x plus b.

35
00:04:35,500 --> 00:04:47,230
This expression here is the derivative of the cost function with respect to w. This expression is the derivative of the cost function with respect to b.

36
00:04:47,230 --> 00:04:54,235
Just as a reminder, you want to update w and b simultaneously on each step.

37
00:04:54,235 --> 00:04:58,085
Now, let's get familiar with how gradient descent works.

38
00:04:58,085 --> 00:05:05,255
One the shoe we saw with gradient descent is that it can lead to a local minimum instead of a global minimum.

39
00:05:05,255 --> 00:05:13,040
Whether global minimum means the point that has the lowest possible value for the cost function J of all possible points.

40
00:05:13,040 --> 00:05:21,575
You may recall this surface plot that looks like an outdoor park with a few hills with the process and the birds as a relaxing Hobo Hill.

41
00:05:21,575 --> 00:05:24,755
This function has more than one local minimum.

42
00:05:24,755 --> 00:05:32,350
Remember, depending on where you initialize the parameters w and b, you can end up at different local minima.

43
00:05:32,350 --> 00:05:36,260
You can end up here, or you can end up here.

44
00:05:36,260 --> 00:05:47,180
But it turns out when you're using a squared error cost function with linear regression, the cost function does not and will never have multiple local minima.

45
00:05:47,180 --> 00:05:51,940
It has a single global minimum because of this bowl-shape.

46
00:05:51,940 --> 00:05:58,525
The technical term for this is that this cost function is a convex function.

47
00:05:58,525 --> 00:06:09,145
Informally, a convex function is of bowl-shaped function and it cannot have any local minima other than the single global minimum.

48
00:06:09,145 --> 00:06:22,235
When you implement gradient descent on a convex function, one nice property is that so long as you're learning rate is chosen appropriately, it will always converge to the global minimum.

49
00:06:22,235 --> 00:06:27,815
Congratulations, you now know how to implement gradient descent for linear regression.

50
00:06:27,815 --> 00:06:30,890
We have just one last video for this week.

51
00:06:30,890 --> 00:06:33,755
That video, we'll see this algorithm in action.

52
00:06:33,755 --> 00:06:36,360
Let's go to that last video.


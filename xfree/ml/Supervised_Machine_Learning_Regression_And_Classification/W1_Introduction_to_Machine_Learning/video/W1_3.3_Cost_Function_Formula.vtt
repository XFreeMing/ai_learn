WEBVTT

1
00:00:00.770 --> 00:00:03.885
In order to implement
linear regression
> 为了实现线性回归

2
00:00:03.885 --> 00:00:05.580
the first key step is first to
> 第一个关键步骤是首先

3
00:00:05.580 --> 00:00:07.935
define something called
a cost function.
> 定义一个称为成本函数的东西。

4
00:00:07.935 --> 00:00:10.155
This is something we'll
build in this video,
> 这是我们将在本视频中构建的东西，

5
00:00:10.155 --> 00:00:13.140
and the cost function
will tell us how well
> 成本函数将告诉我们模型的表现如何
6
00:00:13.140 --> 00:00:15.000
the model is doing so that
> 模型的表现如何

7
00:00:15.000 --> 00:00:17.160
we can try to get
it to do better.
> 我们可以尝试让它做得更好。
8
00:00:17.160 --> 00:00:18.825
Let's look at what this means.
> 让我们看看这意味着什么。

9
00:00:18.825 --> 00:00:21.990
Recall that you have a
training set that contains
> 回想一下，你有一个包含
10
00:00:21.990 --> 00:00:26.040
input features x and
output targets y.
> 输入特征x和输出目标y的训练集。

11
00:00:26.040 --> 00:00:29.365
The model you're going to
use to fit this training set
> 你要用来拟合这个训练集的模型

12
00:00:29.365 --> 00:00:32.535
is this linear function f_w,
> 是这个线性函数f_w，

13
00:00:32.535 --> 00:00:36.525
b of x equals to
w times x plus b.
> b of x等于w乘以x加上b。

14
00:00:36.525 --> 00:00:40.440
To introduce a little bit
more terminology the w
> 为了引入更多术语，w

15
00:00:40.440 --> 00:00:44.250
and b are called the
parameters of the model.
> 和b被称为模型的参数。

16
00:00:44.250 --> 00:00:48.345
In machine learning
parameters of the model are
> 在机器学习中，模型的参数是

17
00:00:48.345 --> 00:00:50.100
the variables you
can adjust during
> 您可以在训练中调整的变量

18
00:00:50.100 --> 00:00:53.385
training in order to
improve the model.
> 以改进模型。

19
00:00:53.385 --> 00:00:57.285
Sometimes you also hear
the parameters w and b
> 有时你也会听到参数w和b

20
00:00:57.285 --> 00:01:01.920
referred to as coefficients
or as weights.
> 被称为系数或权重。

21
00:01:01.920 --> 00:01:04.170
Now let's take a look at what
> 现在让我们看看这些参数w和b

22
00:01:04.170 --> 00:01:07.650
these parameters w and b do.
> 做了什么。

23
00:01:07.650 --> 00:01:10.710
Depending on the values
you've chosen for w and
> 根据您为w和b选择的值

24
00:01:10.710 --> 00:01:14.295
b you get a different
function f of x,
> b，您会得到一个不同的函数f of x，

25
00:01:14.295 --> 00:01:17.050
which generates a different
line on the graph.
> 这在图表上生成不同的线。

26
00:01:17.050 --> 00:01:20.270
Remember that we
can write f of x as
> 记住我们可以将f of x写成

27
00:01:20.270 --> 00:01:24.390
a shorthand for f_w, b of x.
> f_w，b of x的简写。

28
00:01:24.390 --> 00:01:26.670
We're going to take
a look at some plots
> 我们将看一些图表

29
00:01:26.670 --> 00:01:29.325
of f of x on a chart.
> f of x在图表上。

30
00:01:29.325 --> 00:01:31.110
Maybe you're already familiar
> 也许你已经熟悉了

31
00:01:31.110 --> 00:01:32.610
with drawing lines on charts,
> 在图表上画线，

32
00:01:32.610 --> 00:01:34.725
but even if this is
a review for you,
> 但即使这对你来说是一个复习，

33
00:01:34.725 --> 00:01:37.020
I hope this will help
you build intuition
> 我希望这能帮助你建立直觉

34
00:01:37.020 --> 00:01:39.599
on how w and b the parameters
> 关于参数w和b如何

35
00:01:39.599 --> 00:01:47.100
determine f. When w is equal
to 0 and b is equal to 1.5,
> 确定f。当w等于0，b等于1.5，

36
00:01:47.100 --> 00:01:50.400
then f looks like
this horizontal line.
> 那么f看起来像这条水平线。

37
00:01:50.400 --> 00:01:55.335
In this case, the function
f of x is 0 times x
> 在这种情况下，函数f of x是0乘以x

38
00:01:55.335 --> 00:02:00.355
plus 1.5 so f is always
a constant value.
> 加1.5，所以f总是一个常数值。

39
00:02:00.355 --> 00:02:03.270
It always predicts 1.5 for
> 它总是预测1.5

40
00:02:03.270 --> 00:02:08.505
the estimated value of y.
Y hat is always equal to b
> y的估计值。Y hat总是等于b

41
00:02:08.505 --> 00:02:10.725
and here b is also called
> 这里的b也被称为

42
00:02:10.725 --> 00:02:13.020
the y intercept because
that's where it
> y截距，因为那是它

43
00:02:13.020 --> 00:02:17.970
crosses the vertical axis or
the y axis on this graph.
> 在这个图表上穿过垂直轴或y轴。

44
00:02:17.970 --> 00:02:20.354
As a second example,
> 作为第二个例子，

45
00:02:20.354 --> 00:02:24.900
if w is 0.5 and b is equal 0,
> 如果w是0.5，b等于0，

46
00:02:24.900 --> 00:02:28.515
then f of x is 0.5 times x.
> 那么f of x是0.5乘以x。

47
00:02:28.515 --> 00:02:30.090
When x is 0,
> 当x等于0，

48
00:02:30.090 --> 00:02:31.965
the prediction is also 0,
> 预测也是0，

49
00:02:31.965 --> 00:02:33.480
and when x is 2,
> 当x等于2，

50
00:02:33.480 --> 00:02:38.020
then the prediction is
0.5 times 2, which is 1.
> 那么预测是0.5乘以2，即1。

51
00:02:38.020 --> 00:02:41.490
You get a line that looks
like this and notice that
> 你得到一条看起来像这样的线，并注意

52
00:02:41.490 --> 00:02:45.975
the slope is 0.5 divided by 1.
> 斜率是0.5除以1。

53
00:02:45.975 --> 00:02:49.200
The value of w
gives you the slope
> w的值给出了斜率

54
00:02:49.200 --> 00:02:52.695
of the line, which is 0.5.
> 线的斜率，即0.5。

55
00:02:52.695 --> 00:02:58.905
Finally, if w equals
0.5 and b equals 1,
> 最后，如果w等于0.5，b等于1，

56
00:02:58.905 --> 00:03:05.790
then f of x is 0.5 times
x plus 1 and when x is 0,
> 那么f of x是0.5乘以x加1，当x等于0，

57
00:03:05.790 --> 00:03:08.250
then f of x equals b,
> 那么f of x等于b，

58
00:03:08.250 --> 00:03:10.780
which is 1 so the
line intersects
> 即1，所以线与

59
00:03:10.780 --> 00:03:14.285
the vertical axis at
b, the y intercept.
> 垂直轴相交在b，y截距。

60
00:03:14.285 --> 00:03:17.010
Also when x is 2,
> 当x等于2，

61
00:03:17.010 --> 00:03:19.185
then f of x is 2,
> 那么f of x是2，

62
00:03:19.185 --> 00:03:20.860
so the line looks like this.
> 所以线看起来像这样。

63
00:03:20.860 --> 00:03:24.040
Again, this slope
is 0.5 divided by
> 再次，这个斜率是0.5除以

64
00:03:24.040 --> 00:03:28.675
1 so the value of w gives
you the slope which is 0.5.
> 1，所以w的值给出了斜率，即0.5。

65
00:03:28.675 --> 00:03:30.760
Recall that you have
> 回想一下，你有

66
00:03:30.760 --> 00:03:33.415
a training set like
the one shown here.
> 一个像这样的训练集。

67
00:03:33.415 --> 00:03:35.005
With linear regression,
> 使用线性回归，

68
00:03:35.005 --> 00:03:36.940
what you want to do is to choose
> 你想做的是选择

69
00:03:36.940 --> 00:03:38.830
values for the parameters w and
> 参数w和b的值

70
00:03:38.830 --> 00:03:41.110
b so that the straight
line you get from
> 使得从

71
00:03:41.110 --> 00:03:43.930
the function f somehow
fits the data well.
> 函数f得到的直线与数据很好地契合。

72
00:03:43.930 --> 00:03:46.715
Like maybe this line shown here.
> 就像这里显示的线一样。

73
00:03:46.715 --> 00:03:51.040
When I see that the line
fits the data visually,
> 当我看到线与数据视觉上契合时，

74
00:03:51.040 --> 00:03:53.140
you can think of this
to mean that the line
> 你可以认为这意味着线

75
00:03:53.140 --> 00:03:55.840
defined by f is roughly passing
> 由f定义的大致通过

76
00:03:55.840 --> 00:03:59.635
through or somewhere close
to the training examples
> 通过或接近训练示例的线

77
00:03:59.635 --> 00:04:01.990
as compared to other
possible lines
> 与其他可能的线相比

78
00:04:01.990 --> 00:04:04.955
that are not as close
to these points.
> 不如这些点接近。

79
00:04:04.955 --> 00:04:07.910
Just to remind you
of some notation,
> 只是提醒你一些符号，

80
00:04:07.910 --> 00:04:10.580
a training example
like this point
> 一个像这个点的训练示例

81
00:04:10.580 --> 00:04:14.840
here is defined by
x superscript i,
> 这里由x上标i定义，

82
00:04:14.840 --> 00:04:20.180
y superscript i where
y is the target.
> y上标i，其中y是目标。

83
00:04:20.180 --> 00:04:23.350
For a given input x^i,
> 对于给定的输入x^i，

84
00:04:23.350 --> 00:04:28.620
the function f also makes
a predictive value for
> 函数f还为

85
00:04:28.620 --> 00:04:31.200
y and a value that
it predicts to
> y做出预测的值

86
00:04:31.200 --> 00:04:34.575
y is y hat i shown here.
> y是这里显示的y hat i。

87
00:04:34.575 --> 00:04:41.130
For our choice of a model f
of x^i is w times x^i plus b.
> 对于我们选择的模型f of x^i是w乘以x^i加上b。

88
00:04:41.130 --> 00:04:44.835
Stated differently,
the prediction y hat i
> 换句话说，预测y hat i

89
00:04:44.835 --> 00:04:49.155
is f of wb of x^i where
> 是f of wb of x^i，其中

90
00:04:49.155 --> 00:04:52.725
for the model we're using f
> 对于我们使用的模型f

91
00:04:52.725 --> 00:04:58.390
of x^i is equal to wx^i plus b.
> of x^i等于wx^i加b。

92
00:04:58.930 --> 00:05:03.600
Now the question is how
do you find values for
> 现在的问题是如何找到

93
00:05:03.600 --> 00:05:07.920
w and b so that the
prediction y hat i is
> w和b的值，使得预测y hat i

94
00:05:07.920 --> 00:05:11.760
close to the true target y^i for
> 接近真实目标y^i

95
00:05:11.760 --> 00:05:16.860
many or maybe all training
examples x^i, y^i.
> 许多或可能所有训练示例x^i，y^i。

96
00:05:16.860 --> 00:05:18.900
To answer that question,
> 为了回答这个问题，

97
00:05:18.900 --> 00:05:20.830
let's first take
a look at how to
> 让我们首先看一下如何

98
00:05:20.830 --> 00:05:24.430
measure how well a line
fits the training data.
> 测量一条线如何适应训练数据。

99
00:05:24.430 --> 00:05:28.555
To do that, we're going to
construct a cost function.
> 为此，我们将构建一个成本函数。

100
00:05:28.555 --> 00:05:33.250
The cost function takes the
prediction y hat and compares
> 成本函数接受预测y hat并比较

101
00:05:33.250 --> 00:05:39.095
it to the target y by
taking y hat minus y.
> 它与目标y，通过y hat减y。

102
00:05:39.095 --> 00:05:42.360
This difference is
called the error,
> 这个差异被称为错误，

103
00:05:42.360 --> 00:05:44.370
we're measuring how far off to
> 我们正在测量多远

104
00:05:44.370 --> 00:05:47.175
prediction is from the target.
> 预测与目标的差距。

105
00:05:47.175 --> 00:05:52.265
Next, let's computes the
square of this error.
> 接下来，让我们计算这个错误的平方。

106
00:05:52.265 --> 00:05:55.390
Also, we're going to want
to compute this term for
> 此外，我们还要计算这个术语

107
00:05:55.390 --> 00:05:59.065
different training examples
i in the training set.
> 训练集中不同的训练示例i。

108
00:05:59.065 --> 00:06:00.880
When measuring the error,
> 在测量错误时，

109
00:06:00.880 --> 00:06:02.095
for example i,
> 例如i，

110
00:06:02.095 --> 00:06:05.220
we'll compute this
squared error term.
> 我们将计算这个平方误差项。

111
00:06:05.220 --> 00:06:07.310
Finally, we want to measure
> 最后，我们想要测量

112
00:06:07.310 --> 00:06:09.815
the error across the
entire training set.
> 整个训练集的错误。

113
00:06:09.815 --> 00:06:13.700
In particular, let's sum up
the squared errors like this.
> 特别是，让我们这样总结平方误差。

114
00:06:13.700 --> 00:06:16.550
We'll sum from i equals 1,2,
> 我们将从i等于1,2开始求和，

115
00:06:16.550 --> 00:06:18.860
3 all the way up to
> 3一直到

116
00:06:18.860 --> 00:06:23.180
m and remember that m is the
number of training examples,
> m，记住m是训练示例的数量，

117
00:06:23.180 --> 00:06:25.700
which is 47 for this dataset.
> 对于这个数据集，这是47。

118
00:06:25.700 --> 00:06:28.850
Notice that if we have more
training examples m is
> 请注意，如果我们有更多的训练示例m

119
00:06:28.850 --> 00:06:31.100
larger and your cost function
> 更大，你的成本函数

120
00:06:31.100 --> 00:06:32.915
will calculate a bigger number.
> 将计算一个更大的数字。

121
00:06:32.915 --> 00:06:35.765
This is summing
over more examples.
> 这是对更多示例求和。

122
00:06:35.765 --> 00:06:37.970
To build a cost function that
> 要构建一个成本函数

123
00:06:37.970 --> 00:06:39.935
doesn't automatically get bigger
> 不会自动变得更大

124
00:06:39.935 --> 00:06:44.165
as the training set size
gets larger by convention,
> 随着训练集大小的增加而变大，按照惯例，

125
00:06:44.165 --> 00:06:48.200
we will compute the average
squared error instead of
> 我们将计算平均平方误差而不是

126
00:06:48.200 --> 00:06:50.720
the total squared
error and we do
> 总平方误差，我们这样做

127
00:06:50.720 --> 00:06:54.600
that by dividing by m like this.
> 通过这样做来除以m。

128
00:06:54.650 --> 00:06:58.680
We're nearly there.
Just one last thing.
> 我们快到了。最后一件事。

129
00:06:58.680 --> 00:07:00.060
By convention,
> 按照惯例，

130
00:07:00.060 --> 00:07:02.910
the cost function that
machine learning people use
> 机器学习人员使用的成本函数

131
00:07:02.910 --> 00:07:07.650
actually divides by 2 times
m. The extra division
> 实际上除以2乘以m。额外的除法

132
00:07:07.650 --> 00:07:09.690
by 2 is just meant
to make some of
> 2的额外除法只是为了使一些

133
00:07:09.690 --> 00:07:12.540
our later calculations
look neater,
> 我们后来的计算看起来更整洁，

134
00:07:12.540 --> 00:07:14.580
but the cost function
still works whether you
> 但成本函数仍然有效，无论你

135
00:07:14.580 --> 00:07:17.130
include this division
by 2 or not.
> 包括这个除以2的除法还是不包括。

136
00:07:17.130 --> 00:07:18.840
This expression right here is
> 这里的表达式是

137
00:07:18.840 --> 00:07:21.645
the cost function and
we're going to write
> 成本函数，我们将写

138
00:07:21.645 --> 00:07:27.470
J of wb to refer to
the cost function.
> J of wb来指代成本函数。

139
00:07:27.470 --> 00:07:31.795
This is also called the
squared error cost function,
> 这也被称为平方误差成本函数，

140
00:07:31.795 --> 00:07:34.000
and it's called this
because you're taking
> 这样称呼是因为你正在

141
00:07:34.000 --> 00:07:36.800
the square of these error terms.
> 这些错误项的平方。

142
00:07:36.800 --> 00:07:39.790
In machine learning
different people
> 在机器学习中，不同的人

143
00:07:39.790 --> 00:07:41.440
will use different
cost functions
> 将使用不同的成本函数

144
00:07:41.440 --> 00:07:42.819
for different applications,
> 用于不同的应用，

145
00:07:42.819 --> 00:07:46.630
but the squared error cost
function is by far the most
> 但平方误差成本函数是迄今为止最常用的

146
00:07:46.630 --> 00:07:48.310
commonly used one for
> 用于

147
00:07:48.310 --> 00:07:50.860
linear regression
and for that matter,
> 线性回归和此外，

148
00:07:50.860 --> 00:07:53.170
for all regression
problems where it
> 对于所有回归问题，它

149
00:07:53.170 --> 00:07:56.375
seems to give good results
for many applications.
> 似乎为许多应用提供了良好的结果。

150
00:07:56.375 --> 00:08:00.775
Just as a reminder,
the prediction y hat
> 只是作为提醒，预测y hat

151
00:08:00.775 --> 00:08:05.615
is equal to the outputs
of the model f at x.
> 等于模型f在x处的输出。

152
00:08:05.615 --> 00:08:09.600
We can rewrite the
cost function J of
> 我们可以重写成本函数J of

153
00:08:09.600 --> 00:08:14.010
wb as 1 over 2m times
> wb为1除以2m乘以

154
00:08:14.010 --> 00:08:17.820
the sum from i
equals 1 to m of f
> 从i等于1到m的f的总和

155
00:08:17.820 --> 00:08:23.050
of x^i minus y^i the
quantity squared.
> x^i减y^i的数量平方。

156
00:08:23.050 --> 00:08:26.285
Eventually we're going to
want to find values of
> 最终，我们将要找到

157
00:08:26.285 --> 00:08:29.630
w and b that make the
cost function small.
> 使成本函数变小的w和b的值。

158
00:08:29.630 --> 00:08:31.775
But before going there,
> 但在那之前，

159
00:08:31.775 --> 00:08:34.670
let's first gain more
intuition about what
> 让我们首先更多地了解

160
00:08:34.670 --> 00:08:38.105
J of wb is really computing.
> J of wb真正计算的是什么。

161
00:08:38.105 --> 00:08:40.730
At this point you might
be thinking we've done
> 此时，你可能会认为我们已经完成了

162
00:08:40.730 --> 00:08:43.880
a whole lot of math to
define the cost function.
> 定义成本函数的大量数学。

163
00:08:43.880 --> 00:08:46.280
But what exactly is it doing?
> 但它到底在做什么？

164
00:08:46.280 --> 00:08:49.010
Let's go on to the next
video where we'll step
> 让我们继续下一个视频，我们将迈出

165
00:08:49.010 --> 00:08:51.770
through one example of
what the cost function
> 通过一个例子，成本函数

166
00:08:51.770 --> 00:08:53.750
is really computing
that I hope will
> 真正计算的是我希望

167
00:08:53.750 --> 00:08:56.190
help you build
intuition about what it
> 帮助你建立直觉，它

168
00:08:56.190 --> 00:09:01.605
means if J of wb is large
versus if the cost j is small.
> 如果J of wb很大，与成本j很小相比。

169
00:09:01.605 --> 00:09:04.510
Let's go on to the next video.
> 让我们继续下一个视频。
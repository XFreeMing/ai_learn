{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 pytorch 构建一个 Transformer \n",
    "\n",
    "import copy\n",
    "import math\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "\n",
    "\n",
    "def clones(module, n):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, feature, eps=1e-6):\n",
    "        \"\"\"\n",
    "        :param feature: self-attention 的 x 的大小\n",
    "        :param eps:\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(feature))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(feature))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    这不仅仅做了残差，这是把残差和 layernorm 一起给做了\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout=0.1):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 第一步做 layernorm\n",
    "        self.layer_norm = LayerNorm(size)\n",
    "        # 第二步做 dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        :param x: 就是self-attention的输入\n",
    "        :param sublayer: self-attention层\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.dropout(self.layer_norm(x + sublayer(x)))\n",
    "\n",
    "\n",
    "class FeatEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_feat, d_model, dropout):\n",
    "        super(FeatEmbedding, self).__init__()\n",
    "        self.video_embeddings = nn.Sequential(\n",
    "            LayerNorm(d_feat),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_feat, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.video_embeddings(x)\n",
    "\n",
    "\n",
    "class TextEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, dropout, max_len=5000):\n",
    "        if dim % 2 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                             \"odd dim (got dim={:d})\".format(dim))\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                              -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.drop_out = nn.Dropout(p=dropout)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        if step is None:\n",
    "            emb = emb + self.pe[:emb.size(0)]\n",
    "        else:\n",
    "            emb = emb + self.pe[step]\n",
    "        emb = self.drop_out(emb)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def self_attention(query, key, value, dropout=None, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # mask的操作在QK之后，softmax之前\n",
    "    if mask is not None:\n",
    "        mask.cuda()\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    self_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        self_attn = dropout(self_attn)\n",
    "    return torch.matmul(self_attn, value), self_attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, head, d_model, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert (d_model % head == 0)\n",
    "        self.d_k = d_model // head\n",
    "        self.head = head\n",
    "        self.d_model = d_model # 512 值是个数字 一般代表的是词向量的维度\n",
    "        # nn.Linear(input,output) 是一个线性变换层 output=input⋅weight^T +bias\n",
    "        # weight 是权重矩阵\n",
    "        self.linear_query = nn.Linear(d_model, d_model) \n",
    "        self.linear_key = nn.Linear(d_model, d_model)\n",
    "        self.linear_value = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attn = None\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # 多头注意力机制的线性变换层是4维，是把query[batch, frame_num, d_model]变成[batch, -1, head, d_k]\n",
    "            # 再1，2维交换变成[batch, head, -1, d_k], 所以mask要在第一维添加一维，与后面的self attention计算维度一样\n",
    "            mask = mask.unsqueeze(1)\n",
    "        n_batch = query.size(0)\n",
    "        # if self.head == 1:\n",
    "        #     x, self.attn = self_attention(query, key, value, dropout=self.dropout, mask=mask)\n",
    "        # else:\n",
    "        #     query = self.linear_query(query).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 32, 64]\n",
    "        #     key = self.linear_key(key).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 28, 64]\n",
    "        #     value = self.linear_value(value).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 28, 64]\n",
    "        #\n",
    "        #     x, self.attn = self_attention(query, key, value, dropout=self.dropout, mask=mask)\n",
    "        #     # 变为三维， 或者说是concat head\n",
    "        #     x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.head * self.d_k)\n",
    "\n",
    "        query = self.linear_query(query).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 32, 64]\n",
    "        key = self.linear_key(key).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 28, 64]\n",
    "        value = self.linear_value(value).view(n_batch, -1, self.head, self.d_k).transpose(1, 2)  # [b, 8, 28, 64]\n",
    "\n",
    "        x, self.attn = self_attention(query, key, value, dropout=self.dropout, mask=mask)\n",
    "        # 变为三维， 或者说是concat head\n",
    "        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.head * self.d_k)\n",
    "\n",
    "        return self.linear_out(x)\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inter = self.dropout_1(self.relu(self.w_1(self.layer_norm(x))))\n",
    "        output = self.dropout_2(self.w_2(inter))\n",
    "        return output\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, attn, feed_forward, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attn = attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_connection = clones(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer_connection[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        return self.sublayer_connection[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class EncoderLayerNoAttention(nn.Module):\n",
    "    def __init__(self, size, attn, feed_forward, dropout=0.1):\n",
    "        super(EncoderLayerNoAttention, self).__init__()\n",
    "        self.attn = attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_connection = clones(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        return self.sublayer_connection[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, size, attn, feed_forward, sublayer_num, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attn = attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_connection = clones(SublayerConnection(size, dropout), sublayer_num)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, trg_mask, r2l_memory=None, r2l_trg_mask=None):\n",
    "        x = self.sublayer_connection[0](x, lambda x: self.attn(x, x, x, trg_mask))\n",
    "        x = self.sublayer_connection[1](x, lambda x: self.attn(x, memory, memory, src_mask))\n",
    "\n",
    "        if r2l_memory is not None:\n",
    "            x = self.sublayer_connection[-2](x, lambda x: self.attn(x, r2l_memory, r2l_memory, r2l_trg_mask))\n",
    "\n",
    "        return self.sublayer_connection[-1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n, encoder_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layer = clones(encoder_layer, n)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        for layer in self.encoder_layer:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class R2L_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n, decoder_layer):\n",
    "        super(R2L_Decoder, self).__init__()\n",
    "        self.decoder_layer = clones(decoder_layer, n)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, r2l_trg_mask):\n",
    "        for layer in self.decoder_layer:\n",
    "            x = layer(x, memory, src_mask, r2l_trg_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class L2R_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n, decoder_layer):\n",
    "        super(L2R_Decoder, self).__init__()\n",
    "        self.decoder_layer = clones(decoder_layer, n)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask):\n",
    "        for layer in self.decoder_layer:\n",
    "            x = layer(x, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "def pad_mask(src, r2l_trg, trg, pad_idx):\n",
    "    if isinstance(src, tuple):\n",
    "        if len(src) == 4:\n",
    "            src_image_mask = (src[0][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            src_motion_mask = (src[1][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            src_object_mask = (src[2][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            src_rel_mask = (src[3][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            enc_src_mask = (src_image_mask, src_motion_mask, src_object_mask, src_rel_mask)\n",
    "            dec_src_mask_1 = src_image_mask & src_motion_mask\n",
    "            dec_src_mask_2 = src_image_mask & src_motion_mask & src_object_mask & src_rel_mask\n",
    "            dec_src_mask = (dec_src_mask_1, dec_src_mask_2)\n",
    "            src_mask = (enc_src_mask, dec_src_mask)\n",
    "        if len(src) == 3:\n",
    "            src_image_mask = (src[0][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            src_motion_mask = (src[1][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            src_object_mask = (src[2][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            enc_src_mask = (src_image_mask, src_motion_mask, src_object_mask)\n",
    "            dec_src_mask = src_image_mask & src_motion_mask\n",
    "            src_mask = (enc_src_mask, dec_src_mask)\n",
    "        if len(src) == 2:\n",
    "            src_image_mask = (src[0][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            src_motion_mask = (src[1][:, :, 0] != pad_idx).unsqueeze(1)\n",
    "            enc_src_mask = (src_image_mask, src_motion_mask)\n",
    "            dec_src_mask = src_image_mask & src_motion_mask\n",
    "            src_mask = (enc_src_mask, dec_src_mask)\n",
    "    else:\n",
    "        src_mask = (src[:, :, 0] != pad_idx).unsqueeze(1)\n",
    "    if trg is not None:\n",
    "        if isinstance(src_mask, tuple):\n",
    "            trg_mask = (trg != pad_idx).unsqueeze(1) & subsequent_mask(trg.size(1)).type_as(src_image_mask.data)\n",
    "            r2l_pad_mask = (r2l_trg != pad_idx).unsqueeze(1).type_as(src_image_mask.data)\n",
    "            r2l_trg_mask = r2l_pad_mask & subsequent_mask(r2l_trg.size(1)).type_as(src_image_mask.data)\n",
    "            return src_mask, r2l_pad_mask, r2l_trg_mask, trg_mask\n",
    "        else:\n",
    "            trg_mask = (trg != pad_idx).unsqueeze(1) & subsequent_mask(trg.size(1)).type_as(src_mask.data)\n",
    "            r2l_pad_mask = (r2l_trg != pad_idx).unsqueeze(1).type_as(src_mask.data)\n",
    "            r2l_trg_mask = r2l_pad_mask & subsequent_mask(r2l_trg.size(1)).type_as(src_mask.data)\n",
    "            return src_mask, r2l_pad_mask, r2l_trg_mask, trg_mask  # src_mask[batch, 1, lens]  trg_mask[batch, 1, lens]\n",
    "\n",
    "    else:\n",
    "        return src_mask\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"Mask out subsequent positions.\"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return (torch.from_numpy(mask) == 0).cuda()\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)\n",
    "\n",
    "\n",
    "class ABDTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, d_feat, d_model, d_ff, n_heads, n_layers, dropout, feature_mode,\n",
    "                 device='cuda', n_heads_big=128):\n",
    "        super(ABDTransformer, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self.feature_mode = feature_mode\n",
    "\n",
    "        c = copy.deepcopy\n",
    "\n",
    "        # attn_no_heads = MultiHeadAttention(1, d_model, dropout)\n",
    "\n",
    "        attn = MultiHeadAttention(n_heads, d_model, dropout)\n",
    "\n",
    "        attn_big = MultiHeadAttention(n_heads_big, d_model, dropout)\n",
    "\n",
    "        # attn_big2 = MultiHeadAttention(10, d_model, dropout)\n",
    "\n",
    "        feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        if feature_mode == 'one':\n",
    "            self.src_embed = FeatEmbedding(d_feat, d_model, dropout)\n",
    "        elif feature_mode == 'two':\n",
    "            self.image_src_embed = FeatEmbedding(d_feat[0], d_model, dropout)\n",
    "            self.motion_src_embed = FeatEmbedding(d_feat[1], d_model, dropout)\n",
    "        elif feature_mode == 'three':\n",
    "            self.image_src_embed = FeatEmbedding(d_feat[0], d_model, dropout)\n",
    "            self.motion_src_embed = FeatEmbedding(d_feat[1], d_model, dropout)\n",
    "            self.object_src_embed = FeatEmbedding(d_feat[2], d_model, dropout)\n",
    "        elif feature_mode == 'four':\n",
    "            self.image_src_embed = FeatEmbedding(d_feat[0], d_model, dropout)\n",
    "            self.motion_src_embed = FeatEmbedding(d_feat[1], d_model, dropout)\n",
    "            self.object_src_embed = FeatEmbedding(d_feat[2], d_model, dropout)\n",
    "            self.rel_src_embed = FeatEmbedding(d_feat[3], d_model, dropout)\n",
    "        self.trg_embed = TextEmbedding(vocab.n_vocabs, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # self.encoder_no_heads = Encoder(n_layers, EncoderLayer(d_model, c(attn_no_heads), c(feed_forward), dropout))\n",
    "\n",
    "        self.encoder = Encoder(n_layers, EncoderLayer(d_model, c(attn), c(feed_forward), dropout))\n",
    "\n",
    "        self.encoder_big = Encoder(n_layers, EncoderLayer(d_model, c(attn_big), c(feed_forward), dropout))\n",
    "\n",
    "        # self.encoder_big2 = Encoder(n_layers, EncoderLayer(d_model, c(attn_big2), c(feed_forward), dropout))\n",
    "\n",
    "        self.encoder_no_attention = Encoder(n_layers,\n",
    "                                            EncoderLayerNoAttention(d_model, c(attn), c(feed_forward), dropout))\n",
    "\n",
    "        self.r2l_decoder = R2L_Decoder(n_layers, DecoderLayer(d_model, c(attn), c(feed_forward),\n",
    "                                                              sublayer_num=3, dropout=dropout))\n",
    "        self.l2r_decoder = L2R_Decoder(n_layers, DecoderLayer(d_model, c(attn), c(feed_forward),\n",
    "                                                              sublayer_num=4, dropout=dropout))\n",
    "\n",
    "        self.generator = Generator(d_model, vocab.n_vocabs)\n",
    "\n",
    "    def encode(self, src, src_mask, feature_mode_two=False):\n",
    "        if self.feature_mode == 'two':\n",
    "            x1 = self.image_src_embed(src[0])\n",
    "            x1 = self.pos_embed(x1)\n",
    "            x1 = self.encoder_big(x1, src_mask[0])\n",
    "            x2 = self.motion_src_embed(src[1])\n",
    "            x2 = self.pos_embed(x2)\n",
    "            x2 = self.encoder_big(x2, src_mask[1])\n",
    "            return x1 + x2\n",
    "        if feature_mode_two:\n",
    "            x1 = self.image_src_embed(src[0])\n",
    "            x1 = self.pos_embed(x1)\n",
    "            x1 = self.encoder_big(x1, src_mask[0])\n",
    "            x2 = self.motion_src_embed(src[1])\n",
    "            x2 = self.pos_embed(x2)\n",
    "            x2 = self.encoder_big(x2, src_mask[1])\n",
    "            return x1 + x2\n",
    "        if self.feature_mode == 'one':\n",
    "            x = self.src_embed(src)\n",
    "            x = self.pos_embed(x)\n",
    "            return self.encoder(x, src_mask)\n",
    "        elif self.feature_mode == 'two':\n",
    "            x1 = self.image_src_embed(src[0])\n",
    "            x1 = self.pos_embed(x1)\n",
    "            x1 = self.encoder_big(x1, src_mask[0])\n",
    "            x2 = self.motion_src_embed(src[1])\n",
    "            x2 = self.pos_embed(x2)\n",
    "            x2 = self.encoder_big(x2, src_mask[1])\n",
    "            return x1 + x2\n",
    "        elif self.feature_mode == 'three':\n",
    "            x1 = self.image_src_embed(src[0])\n",
    "            x1 = self.pos_embed(x1)\n",
    "            x1 = self.encoder(x1, src_mask[0])\n",
    "            x2 = self.motion_src_embed(src[1])\n",
    "            x2 = self.pos_embed(x2)\n",
    "            x2 = self.encoder(x2, src_mask[1])\n",
    "            x3 = self.object_src_embed(src[2])\n",
    "            x3 = self.pos_embed(x3)\n",
    "            x3 = self.encoder(x3, src_mask[2])\n",
    "            return x1 + x2 + x3\n",
    "        elif self.feature_mode == 'four':\n",
    "            x1 = self.image_src_embed(src[0])\n",
    "            x1 = self.pos_embed(x1)\n",
    "            x1 = self.encoder(x1, src_mask[0])\n",
    "\n",
    "            x2 = self.motion_src_embed(src[1])\n",
    "            x2 = self.pos_embed(x2)\n",
    "            x2 = self.encoder(x2, src_mask[1])\n",
    "\n",
    "            x3 = self.object_src_embed(src[2])\n",
    "            # x3 = self.pos_embed(x3)\n",
    "            x3 = self.encoder(x3, src_mask[2])\n",
    "            # x3 = self.encoder_no_attention(x3, src_mask[2])\n",
    "\n",
    "            x4 = self.rel_src_embed(src[3])\n",
    "            # x4 = self.pos_embed(x4)\n",
    "            # x4 = self.encoder_no_\n",
    "            # heads(x4, src_mask[3])\n",
    "            x4 = self.encoder_no_attention(x4, src_mask[3])\n",
    "            # x4 = self.encoder(x4, src_mask[3])\n",
    "            return x1 + x2 + x3 + x4\n",
    "\n",
    "    def r2l_decode(self, r2l_trg, memory, src_mask, r2l_trg_mask):\n",
    "        x = self.trg_embed(r2l_trg)\n",
    "        x = self.pos_embed(x)\n",
    "        return self.r2l_decoder(x, memory, src_mask, r2l_trg_mask)\n",
    "\n",
    "    def l2r_decode(self, trg, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask):\n",
    "        x = self.trg_embed(trg)\n",
    "        x = self.pos_embed(x)\n",
    "        return self.l2r_decoder(x, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask)\n",
    "\n",
    "    def forward(self, src, r2l_trg, trg, mask):\n",
    "        src_mask, r2l_pad_mask, r2l_trg_mask, trg_mask = mask\n",
    "        if self.feature_mode == 'one':\n",
    "            encoding_outputs = self.encode(src, src_mask)\n",
    "            r2l_outputs = self.r2l_decode(r2l_trg, encoding_outputs, src_mask, r2l_trg_mask)\n",
    "            l2r_outputs = self.l2r_decode(trg, encoding_outputs, src_mask, trg_mask, r2l_outputs, r2l_pad_mask)\n",
    "\n",
    "        elif self.feature_mode == 'two' or 'three' or 'four':\n",
    "            enc_src_mask, dec_src_mask = src_mask\n",
    "            r2l_encoding_outputs = self.encode(src, enc_src_mask, feature_mode_two=True)\n",
    "            encoding_outputs = self.encode(src, enc_src_mask)\n",
    "\n",
    "            r2l_outputs = self.r2l_decode(r2l_trg, r2l_encoding_outputs, dec_src_mask[0], r2l_trg_mask)\n",
    "            l2r_outputs = self.l2r_decode(trg, encoding_outputs, dec_src_mask[1], trg_mask, r2l_outputs, r2l_pad_mask)\n",
    "\n",
    "            # r2l_outputs = self.r2l_decode(r2l_trg, encoding_outputs, dec_src_mask, r2l_trg_mask)\n",
    "            # l2r_outputs = self.l2r_decode(trg, encoding_outputs, dec_src_mask, trg_mask, None, None)\n",
    "        else:\n",
    "            raise \"没有输出\"\n",
    "\n",
    "        r2l_pred = self.generator(r2l_outputs)\n",
    "        l2r_pred = self.generator(l2r_outputs)\n",
    "\n",
    "        return r2l_pred, l2r_pred\n",
    "\n",
    "    def greedy_decode(self, batch_size, src_mask, memory, max_len):\n",
    "\n",
    "        eos_idx = self.vocab.word2idx['<S>']\n",
    "        r2l_hidden = None\n",
    "        with torch.no_grad():\n",
    "            output = torch.ones(batch_size, 1).fill_(eos_idx).long().cuda()\n",
    "            for i in range(max_len + 2 - 1):\n",
    "                trg_mask = subsequent_mask(output.size(1))\n",
    "                dec_out = self.r2l_decode(output, memory, src_mask, trg_mask)  # batch, len, d_model\n",
    "                r2l_hidden = dec_out\n",
    "                pred = self.generator(dec_out)  # batch, len, n_vocabs\n",
    "                next_word = pred[:, -1].max(dim=-1)[1].unsqueeze(1)  # pred[:, -1]([batch, n_vocabs])\n",
    "                output = torch.cat([output, next_word], dim=-1)\n",
    "        return r2l_hidden, output\n",
    "\n",
    "    # beam search 必用的\n",
    "    def r2l_beam_search_decode(self, batch_size, src, src_mask, model_encodings, beam_size, max_len):\n",
    "        end_symbol = self.vocab.word2idx['<S>']\n",
    "        start_symbol = self.vocab.word2idx['<S>']\n",
    "\n",
    "        r2l_outputs = None\n",
    "\n",
    "        # 1.1 Setup Src\n",
    "        \"src has shape (batch_size, sent_len)\"\n",
    "        \"src_mask has shape (batch_size, 1, sent_len)\"\n",
    "        # src_mask = (src[:, :, 0] != self.vocab.word2idx['<PAD>']).unsqueeze(-2)  # TODO Untested\n",
    "        \"model_encodings has shape (batch_size, sentence_len, d_model)\"\n",
    "        # model_encodings = self.encode(src, src_mask)\n",
    "\n",
    "        # 1.2 Setup Tgt Hypothesis Tracking\n",
    "        \"hypothesis is List(4 bt)[(cur beam_sz, dec_sent_len)], init: List(4 bt)[(1 init_beam_sz, dec_sent_len)]\"\n",
    "        \"hypotheses[i] is shape (cur beam_sz, dec_sent_len)\"\n",
    "        hypotheses = [copy.deepcopy(torch.full((1, 1), start_symbol, dtype=torch.long,\n",
    "                                               device=self.device)) for _ in range(batch_size)]\n",
    "        \"List after init: List 4 bt of List of len max_len_completed, init: List of len 4 bt of []\"\n",
    "        completed_hypotheses = [copy.deepcopy([]) for _ in range(batch_size)]\n",
    "        \"List len batch_sz of shape (cur beam_sz), init: List(4 bt)[(1 init_beam_sz)]\"\n",
    "        \"hyp_scores[i] is shape (cur beam_sz)\"\n",
    "        hyp_scores = [copy.deepcopy(torch.full((1,), 0, dtype=torch.float, device=self.device))\n",
    "                      for _ in range(batch_size)]  # probs are log_probs must be init at 0.\n",
    "\n",
    "        # 2. Iterate: Generate one char at a time until maxlen\n",
    "        for iter in range(max_len + 1):\n",
    "            if all([len(completed_hypotheses[i]) == beam_size for i in range(batch_size)]):\n",
    "                break\n",
    "\n",
    "            # 2.1 Setup the batch. Since we use beam search, each batch has a variable number (called cur_beam_size)\n",
    "            # between 0 and beam_size of hypotheses live at any moment. We decode all hypotheses for all batches at\n",
    "            # the same time, so we must copy the src_encodings, src_mask, etc the appropriate number fo times for\n",
    "            # the number of hypotheses for each example. We keep track of the number of live hypotheses for each example.\n",
    "            # We run all hypotheses for all examples together through the decoder and log-softmax,\n",
    "            # and then use `torch.split` to get the appropriate number of hypotheses for each example in the end.\n",
    "            cur_beam_sizes, last_tokens, model_encodings_l, src_mask_l = [], [], [], []\n",
    "            for i in range(batch_size):\n",
    "                if hypotheses[i] is None:\n",
    "                    cur_beam_sizes += [0]\n",
    "                    continue\n",
    "                cur_beam_size, decoded_len = hypotheses[i].shape\n",
    "                cur_beam_sizes += [cur_beam_size]\n",
    "                last_tokens += [hypotheses[i]]\n",
    "                model_encodings_l += [model_encodings[i:i + 1]] * cur_beam_size\n",
    "                src_mask_l += [src_mask[i:i + 1]] * cur_beam_size\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)\"\n",
    "            model_encodings_cur = torch.cat(model_encodings_l, dim=0)\n",
    "            src_mask_cur = torch.cat(src_mask_l, dim=0)\n",
    "            y_tm1 = torch.cat(last_tokens, dim=0)\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)\"\n",
    "            if self.feature_mode == 'one':\n",
    "                out = self.r2l_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,\n",
    "                                      Variable(subsequent_mask(y_tm1.size(-1)).type_as(src.data)).to(self.device))\n",
    "            elif self.feature_mode == 'two' or 'three' or 'four':\n",
    "                out = self.r2l_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,\n",
    "                                      Variable(subsequent_mask(y_tm1.size(-1)).type_as(src[0].data)).to(self.device))\n",
    "            r2l_outputs = out\n",
    "\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)\"\n",
    "            log_prob = self.generator(out[:, -1, :]).unsqueeze(1)\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)\"\n",
    "            _, decoded_len, vocab_sz = log_prob.shape\n",
    "            # log_prob = log_prob.reshape(batch_size, cur_beam_size, decoded_len, vocab_sz)\n",
    "            \"shape List(4 bt)[(cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)]\"\n",
    "            \"log_prob[i] is (cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)\"\n",
    "            log_prob = torch.split(log_prob, cur_beam_sizes, dim=0)\n",
    "\n",
    "            # 2.2 Now we process each example in the batch. Note that the example may have already finished processing before\n",
    "            # other examples (no more hypotheses to try), in which case we continue\n",
    "            new_hypotheses, new_hyp_scores = [], []\n",
    "            for i in range(batch_size):\n",
    "                if hypotheses[i] is None or len(completed_hypotheses[i]) >= beam_size:\n",
    "                    new_hypotheses += [None]\n",
    "                    new_hyp_scores += [None]\n",
    "                    continue\n",
    "\n",
    "                # 2.2.1 We compute the cumulative scores for each live hypotheses for the example\n",
    "                # hyp_scores is the old scores for the previous stage, and `log_prob` are the new probs for\n",
    "                # this stage. Since they are log probs, we sum them instaed of multiplying them.\n",
    "                # The .view(-1) forces all the hypotheses into one dimension. The shape of this dimension is\n",
    "                # cur_beam_sz * vocab_sz (ex: 5 * 50002). So after getting the topk from it, we can recover the\n",
    "                # generating sentence and the next word using: ix // vocab_sz, ix % vocab_sz.\n",
    "                cur_beam_sz_i, dec_sent_len, vocab_sz = log_prob[i].shape\n",
    "                \"shape (vocab_sz,)\"\n",
    "                cumulative_hyp_scores_i = (hyp_scores[i].unsqueeze(-1).unsqueeze(-1)\n",
    "                                           .expand((cur_beam_sz_i, 1, vocab_sz)) + log_prob[i]).view(-1)\n",
    "\n",
    "                # 2.2.2 We get the topk values in cumulative_hyp_scores_i and compute the current (generating) sentence\n",
    "                # and the next word using: ix // vocab_sz, ix % vocab_sz.\n",
    "                \"shape (cur_beam_sz,)\"\n",
    "                live_hyp_num_i = beam_size - len(completed_hypotheses[i])\n",
    "                \"shape (cur_beam_sz,). Vals are between 0 and 50002 vocab_sz\"\n",
    "                top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(cumulative_hyp_scores_i, k=live_hyp_num_i)\n",
    "                \"shape (cur_beam_sz,). prev_hyp_ids vals are 0 <= val < cur_beam_sz. hyp_word_ids vals are 0 <= val < vocab_len\"\n",
    "                prev_hyp_ids, hyp_word_ids = top_cand_hyp_pos // self.vocab.n_vocabs, \\\n",
    "                                             top_cand_hyp_pos % self.vocab.n_vocabs\n",
    "\n",
    "                # 2.2.3 For each of the topk words, we append the new word to the current (generating) sentence\n",
    "                # We add this to new_hypotheses_i and add its corresponding total score to new_hyp_scores_i\n",
    "                new_hypotheses_i, new_hyp_scores_i = [], []  # Removed live_hyp_ids_i, which is used in the LSTM decoder to track live hypothesis ids\n",
    "                for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids,\n",
    "                                                                        top_cand_hyp_scores):\n",
    "                    prev_hyp_id, hyp_word_id, cand_new_hyp_score = \\\n",
    "                        prev_hyp_id.item(), hyp_word_id.item(), cand_new_hyp_score.item()\n",
    "\n",
    "                    new_hyp_sent = torch.cat(\n",
    "                        (hypotheses[i][prev_hyp_id], torch.tensor([hyp_word_id], device=self.device)))\n",
    "                    if hyp_word_id == end_symbol:\n",
    "                        completed_hypotheses[i].append(Hypothesis(\n",
    "                            value=[self.vocab.idx2word[a.item()] for a in new_hyp_sent[1:-1]],\n",
    "                            score=cand_new_hyp_score))\n",
    "                    else:\n",
    "                        new_hypotheses_i.append(new_hyp_sent.unsqueeze(-1))\n",
    "                        new_hyp_scores_i.append(cand_new_hyp_score)\n",
    "\n",
    "                # 2.2.4 We may find that the hypotheses_i for some example in the batch\n",
    "                # is empty - we have fully processed that example. We use None as a sentinel in this case.\n",
    "                # Above, the loops gracefully handle None examples.\n",
    "                if len(new_hypotheses_i) > 0:\n",
    "                    hypotheses_i = torch.cat(new_hypotheses_i, dim=-1).transpose(0, -1).to(self.device)\n",
    "                    hyp_scores_i = torch.tensor(new_hyp_scores_i, dtype=torch.float, device=self.device)\n",
    "                else:\n",
    "                    hypotheses_i, hyp_scores_i = None, None\n",
    "                new_hypotheses += [hypotheses_i]\n",
    "                new_hyp_scores += [hyp_scores_i]\n",
    "            # print(new_hypotheses, new_hyp_scores)\n",
    "            hypotheses, hyp_scores = new_hypotheses, new_hyp_scores\n",
    "\n",
    "        # 2.3 Finally, we do some postprocessing to get our final generated candidate sentences.\n",
    "        # Sometimes, we may get to max_len of a sentence and still not generate the </s> end token.\n",
    "        # In this case, the partial sentence we have generated will not be added to the completed_hypotheses\n",
    "        # automatically, and we have to manually add it in. We add in as many as necessary so that there are\n",
    "        # `beam_size` completed hypotheses for each example.\n",
    "        # Finally, we sort each completed hypothesis by score.\n",
    "        for i in range(batch_size):\n",
    "            hyps_to_add = beam_size - len(completed_hypotheses[i])\n",
    "            if hyps_to_add > 0:\n",
    "                scores, ix = torch.topk(hyp_scores[i], k=hyps_to_add)\n",
    "                for score, id in zip(scores, ix):\n",
    "                    completed_hypotheses[i].append(Hypothesis(\n",
    "                        value=[self.vocab.idx2word[a.item()] for a in hypotheses[i][id][1:]],\n",
    "                        score=score))\n",
    "            completed_hypotheses[i].sort(key=lambda hyp: hyp.score, reverse=True)\n",
    "        return r2l_outputs, completed_hypotheses\n",
    "\n",
    "    def beam_search_decode(self, src, beam_size, max_len):\n",
    "        \"\"\"\n",
    "                An Implementation of Beam Search for the Transformer Model.\n",
    "                Beam search is performed in a batched manner. Each example in a batch generates `beam_size` hypotheses.\n",
    "                We return a list (len: batch_size) of list (len: beam_size) of Hypothesis, which contain our output decoded sentences\n",
    "                and their scores.\n",
    "                :param src: shape (sent_len, batch_size). Each val is 0 < val < len(vocab_dec). The input tokens to the decoder.\n",
    "                :param max_len: the maximum length to decode\n",
    "                :param beam_size: the beam size to use\n",
    "                :return completed_hypotheses: A List of length batch_size, each containing a List of beam_size Hypothesis objects.\n",
    "                    Hypothesis is a named Tuple, its first entry is \"value\" and is a List of strings which contains the translated word\n",
    "                    (one string is one word token). The second entry is \"score\" and it is the log-prob score for this translated sentence.\n",
    "                Note: Below I note \"4 bt\", \"5 beam_size\" as the shapes of objects. 4, 5 are default values. Actual values may differ.\n",
    "                \"\"\"\n",
    "        # 1. Setup\n",
    "        start_symbol = self.vocab.word2idx['<S>']\n",
    "        end_symbol = self.vocab.word2idx['<S>']\n",
    "\n",
    "        # 1.1 Setup Src\n",
    "        \"src has shape (batch_size, sent_len)\"\n",
    "        \"src_mask has shape (batch_size, 1, sent_len)\"\n",
    "        # src_mask = (src[:, :, 0] != self.vocab.word2idx['<PAD>']).unsqueeze(-2)  # TODO Untested\n",
    "        src_mask = pad_mask(src, r2l_trg=None, trg=None, pad_idx=self.vocab.word2idx['<PAD>'])\n",
    "        \"model_encodings has shape (batch_size, sentence_len, d_model)\"\n",
    "        if self.feature_mode == 'one':\n",
    "            batch_size = src.shape[0]\n",
    "            model_encodings = self.encode(src, src_mask)\n",
    "            r2l_memory, r2l_completed_hypotheses = self.r2l_beam_search_decode(batch_size, src, src_mask,\n",
    "                                                                               model_encodings=model_encodings,\n",
    "                                                                               beam_size=beam_size, max_len=max_len)\n",
    "        elif self.feature_mode == 'two' or 'three' or 'four':\n",
    "            batch_size = src[0].shape[0]\n",
    "            enc_src_mask = src_mask[0]\n",
    "            dec_src_mask = src_mask[1]\n",
    "            r2l_model_encodings = self.encode(src, enc_src_mask, feature_mode_two=True)\n",
    "            # model_encodings = r2l_model_encodings\n",
    "            model_encodings = self.encode(src, enc_src_mask)\n",
    "\n",
    "            r2l_memory, r2l_completed_hypotheses = self.r2l_beam_search_decode(batch_size, src, dec_src_mask[0],\n",
    "                                                                               model_encodings=r2l_model_encodings,\n",
    "                                                                               beam_size=beam_size, max_len=max_len)\n",
    "\n",
    "        # 1.2 Setup r2l target output\n",
    "        # r2l_memory, r2l_completed_hypotheses = self.r2l_beam_search_decode(batch_size, src, src_mask,\n",
    "        #                                                                    model_encodings=model_encodings,\n",
    "        #                                                                    beam_size=1, max_len=max_len)\n",
    "        # r2l_memory, r2l_completed_hypotheses = self.greedy_decode(batch_size, src_mask, model_encodings, max_len)\n",
    "        # beam_r2l_memory = [copy.deepcopy(r2l_memory) for _ in range(beam_size)]\n",
    "        # 1.3 Setup Tgt Hypothesis Tracking\n",
    "        \"hypothesis is List(4 bt)[(cur beam_sz, dec_sent_len)], init: List(4 bt)[(1 init_beam_sz, dec_sent_len)]\"\n",
    "        \"hypotheses[i] is shape (cur beam_sz, dec_sent_len)\"\n",
    "        hypotheses = [copy.deepcopy(torch.full((1, 1), start_symbol, dtype=torch.long,\n",
    "                                               device=self.device)) for _ in range(batch_size)]\n",
    "        \"List after init: List 4 bt of List of len max_len_completed, init: List of len 4 bt of []\"\n",
    "        completed_hypotheses = [copy.deepcopy([]) for _ in range(batch_size)]\n",
    "        \"List len batch_sz of shape (cur beam_sz), init: List(4 bt)[(1 init_beam_sz)]\"\n",
    "        \"hyp_scores[i] is shape (cur beam_sz)\"\n",
    "        hyp_scores = [copy.deepcopy(torch.full((1,), 0, dtype=torch.float, device=self.device))\n",
    "                      for _ in range(batch_size)]  # probs are log_probs must be init at 0.\n",
    "\n",
    "        # 2. Iterate: Generate one char at a time until maxlen\n",
    "        for iter in range(max_len + 1):\n",
    "            if all([len(completed_hypotheses[i]) == beam_size for i in range(batch_size)]):\n",
    "                break\n",
    "\n",
    "            # 2.1 Setup the batch. Since we use beam search, each batch has a variable number (called cur_beam_size)\n",
    "            # between 0 and beam_size of hypotheses live at any moment. We decode all hypotheses for all batches at\n",
    "            # the same time, so we must copy the src_encodings, src_mask, etc the appropriate number fo times for\n",
    "            # the number of hypotheses for each example. We keep track of the number of live hypotheses for each example.\n",
    "            # We run all hypotheses for all examples together through the decoder and log-softmax,\n",
    "            # and then use `torch.split` to get the appropriate number of hypotheses for each example in the end.\n",
    "            cur_beam_sizes, last_tokens, model_encodings_l, src_mask_l, r2l_memory_l = [], [], [], [], []\n",
    "            for i in range(batch_size):\n",
    "                if hypotheses[i] is None:\n",
    "                    cur_beam_sizes += [0]\n",
    "                    continue\n",
    "                cur_beam_size, decoded_len = hypotheses[i].shape\n",
    "                cur_beam_sizes += [cur_beam_size]\n",
    "                last_tokens += [hypotheses[i]]\n",
    "                model_encodings_l += [model_encodings[i:i + 1]] * cur_beam_size\n",
    "                if self.feature_mode == 'one':\n",
    "                    src_mask_l += [src_mask[i:i + 1]] * cur_beam_size\n",
    "                elif self.feature_mode == 'two' or 'three' or 'four':\n",
    "                    src_mask_l += [dec_src_mask[1][i:i + 1]] * cur_beam_size\n",
    "                r2l_memory_l += [r2l_memory[i: i + 1]] * cur_beam_size\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)\"\n",
    "            model_encodings_cur = torch.cat(model_encodings_l, dim=0)\n",
    "            src_mask_cur = torch.cat(src_mask_l, dim=0)\n",
    "            y_tm1 = torch.cat(last_tokens, dim=0)\n",
    "            r2l_memory_cur = torch.cat(r2l_memory_l, dim=0)\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)\"\n",
    "            if self.feature_mode == 'one':\n",
    "                out = self.l2r_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,\n",
    "                                      Variable(subsequent_mask(y_tm1.size(-1)).type_as(src.data)).to(self.device),\n",
    "                                      r2l_memory_cur, r2l_trg_mask=None)\n",
    "            elif self.feature_mode == 'two' or 'three' or 'four':\n",
    "                out = self.l2r_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,\n",
    "                                      Variable(subsequent_mask(y_tm1.size(-1)).type_as(src[0].data)).to(self.device),\n",
    "                                      r2l_memory_cur, r2l_trg_mask=None)\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)\"\n",
    "            log_prob = self.generator(out[:, -1, :]).unsqueeze(1)\n",
    "            \"shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)\"\n",
    "            _, decoded_len, vocab_sz = log_prob.shape\n",
    "            # log_prob = log_prob.reshape(batch_size, cur_beam_size, decoded_len, vocab_sz)\n",
    "            \"shape List(4 bt)[(cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)]\"\n",
    "            \"log_prob[i] is (cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)\"\n",
    "            log_prob = torch.split(log_prob, cur_beam_sizes, dim=0)\n",
    "\n",
    "            # 2.2 Now we process each example in the batch. Note that the example may have already finished processing before\n",
    "            # other examples (no more hypotheses to try), in which case we continue\n",
    "            new_hypotheses, new_hyp_scores = [], []\n",
    "            for i in range(batch_size):\n",
    "                if hypotheses[i] is None or len(completed_hypotheses[i]) >= beam_size:\n",
    "                    new_hypotheses += [None]\n",
    "                    new_hyp_scores += [None]\n",
    "                    continue\n",
    "\n",
    "                # 2.2.1 We compute the cumulative scores for each live hypotheses for the example\n",
    "                # hyp_scores is the old scores for the previous stage, and `log_prob` are the new probs for\n",
    "                # this stage. Since they are log probs, we sum them instaed of multiplying them.\n",
    "                # The .view(-1) forces all the hypotheses into one dimension. The shape of this dimension is\n",
    "                # cur_beam_sz * vocab_sz (ex: 5 * 50002). So after getting the topk from it, we can recover the\n",
    "                # generating sentence and the next word using: ix // vocab_sz, ix % vocab_sz.\n",
    "                cur_beam_sz_i, dec_sent_len, vocab_sz = log_prob[i].shape\n",
    "                \"shape (vocab_sz,)\"\n",
    "                cumulative_hyp_scores_i = (hyp_scores[i].unsqueeze(-1).unsqueeze(-1)\n",
    "                                           .expand((cur_beam_sz_i, 1, vocab_sz)) + log_prob[i]).view(-1)\n",
    "\n",
    "                # 2.2.2 We get the topk values in cumulative_hyp_scores_i and compute the current (generating) sentence\n",
    "                # and the next word using: ix // vocab_sz, ix % vocab_sz.\n",
    "                \"shape (cur_beam_sz,)\"\n",
    "                live_hyp_num_i = beam_size - len(completed_hypotheses[i])\n",
    "                \"shape (cur_beam_sz,). Vals are between 0 and 50002 vocab_sz\"\n",
    "                top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(cumulative_hyp_scores_i, k=live_hyp_num_i)\n",
    "                \"shape (cur_beam_sz,). prev_hyp_ids vals are 0 <= val < cur_beam_sz. hyp_word_ids vals are 0 <= val < vocab_len\"\n",
    "                prev_hyp_ids, hyp_word_ids = top_cand_hyp_pos // self.vocab.n_vocabs, \\\n",
    "                                             top_cand_hyp_pos % self.vocab.n_vocabs\n",
    "\n",
    "                # 2.2.3 For each of the topk words, we append the new word to the current (generating) sentence\n",
    "                # We add this to new_hypotheses_i and add its corresponding total score to new_hyp_scores_i\n",
    "                new_hypotheses_i, new_hyp_scores_i = [], []  # Removed live_hyp_ids_i, which is used in the LSTM decoder to track live hypothesis ids\n",
    "                for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids,\n",
    "                                                                        top_cand_hyp_scores):\n",
    "                    prev_hyp_id, hyp_word_id, cand_new_hyp_score = \\\n",
    "                        prev_hyp_id.item(), hyp_word_id.item(), cand_new_hyp_score.item()\n",
    "\n",
    "                    new_hyp_sent = torch.cat(\n",
    "                        (hypotheses[i][prev_hyp_id], torch.tensor([hyp_word_id], device=self.device)))\n",
    "                    if hyp_word_id == end_symbol:\n",
    "                        completed_hypotheses[i].append(Hypothesis(\n",
    "                            value=[self.vocab.idx2word[a.item()] for a in new_hyp_sent[1:-1]],\n",
    "                            score=cand_new_hyp_score))\n",
    "                    else:\n",
    "                        new_hypotheses_i.append(new_hyp_sent.unsqueeze(-1))\n",
    "                        new_hyp_scores_i.append(cand_new_hyp_score)\n",
    "\n",
    "                # 2.2.4 We may find that the hypotheses_i for some example in the batch\n",
    "                # is empty - we have fully processed that example. We use None as a sentinel in this case.\n",
    "                # Above, the loops gracefully handle None examples.\n",
    "                if len(new_hypotheses_i) > 0:\n",
    "                    hypotheses_i = torch.cat(new_hypotheses_i, dim=-1).transpose(0, -1).to(self.device)\n",
    "                    hyp_scores_i = torch.tensor(new_hyp_scores_i, dtype=torch.float, device=self.device)\n",
    "                else:\n",
    "                    hypotheses_i, hyp_scores_i = None, None\n",
    "                new_hypotheses += [hypotheses_i]\n",
    "                new_hyp_scores += [hyp_scores_i]\n",
    "            # print(new_hypotheses, new_hyp_scores)\n",
    "            hypotheses, hyp_scores = new_hypotheses, new_hyp_scores\n",
    "\n",
    "        # 2.3 Finally, we do some postprocessing to get our final generated candidate sentences.\n",
    "        # Sometimes, we may get to max_len of a sentence and still not generate the </s> end token.\n",
    "        # In this case, the partial sentence we have generated will not be added to the completed_hypotheses\n",
    "        # automatically, and we have to manually add it in. We add in as many as necessary so that there are\n",
    "        # `beam_size` completed hypotheses for each example.\n",
    "        # Finally, we sort each completed hypothesis by score.\n",
    "        for i in range(batch_size):\n",
    "            hyps_to_add = beam_size - len(completed_hypotheses[i])\n",
    "            if hyps_to_add > 0:\n",
    "                scores, ix = torch.topk(hyp_scores[i], k=hyps_to_add)\n",
    "                for score, id in zip(scores, ix):\n",
    "                    completed_hypotheses[i].append(Hypothesis(\n",
    "                        value=[self.vocab.idx2word[a.item()] for a in hypotheses[i][id][1:]],\n",
    "                        score=score))\n",
    "            completed_hypotheses[i].sort(key=lambda hyp: hyp.score, reverse=True)\n",
    "        # print('completed_hypotheses', completed_hypotheses)\n",
    "        return r2l_completed_hypotheses, completed_hypotheses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
